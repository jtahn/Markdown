# refs
- https://course.fast.ai/
	- https://course.fast.ai/Lessons/Summaries/lesson1.html
- https://www.youtube.com/playlist?list=PLfYUBJiXbdtSvpQjSnJJ_PmDQB_VyT5iU
- https://nbviewer.org/github/fastai/fastbook/tree/master/
	- https://nbviewer.org/github/fastai/fastbook/tree/master/clean/

# meta
- only save notes on things that are 'actionable' for writeups
	- concepts that need to be addressed by a writeup
	- ideas to replicate
- and the writeups should only be on things that interviewers care about
	- (bc that's what's most urgent)
	- 'indirectly care' counts as well
		- ie build concepts that i directly need to know for later writeups
- there's no need to save 'theory' notes
	- trust brain/inductive process to vaguely
		- remember if i've seen a concept/context before
		- know where to look to refresh myself
	- the only 'theory' that's urgent to write down, is what is used in the writeups
	- if interviewers don't care about a 'project' that involves that theory; then that theory is not urgent rn


# 1

## lecture

- https://forums.fast.ai/t/duckduckgo-search-not-working/105738?replies_to_post_number=52



## book
https://nbviewer.org/github/fastai/fastbook/blob/master/01_intro.ipynb

- replication ideas
	- practice with the 'artisanal' stuff:
		- The hardest part of deep learning is artisanal: how do you know if you've got enough data, whether it is in the right format, if your model is training properly, and, if it's not, what you should do about it? That is why we believe in learning by doing. As with basic data science skills, with deep learning you only get better through practical experience. Trying to spend too much time on the theory can be counterproductive. The key is to just code and try to solve problems: the theory can come later, when you have context and motivation.
	- understand the all/major parts of fast.ai and of 'high-level' parts of pytorch
		- in particular, understand 'best practices' with using the low-level pytorch stuff
		- apparently 2 of the best libraries rn
			- aka well worth learning, even if 'new libraries' become best in the future
			- a lot of the 'best practices' will surely carry over
		- maybe take a look at the mentioned paper:
			- The fastai library is the most popular library for adding this higher-level functionality on top of PyTorch. It's also particularly well suited to the purposes of this book, because it is unique in providing a deeply layered software architecture (there's even a [peer-reviewed academic paper](https://arxiv.org/abs/2002.04688) about this layered API). In this book, as we go deeper and deeper into the foundations of deep learning, we will also go deeper and deeper into the layers of fastai.
			- By the end of the book, you'll understand nearly all the code that's inside fastai (and much of PyTorch too), because in each chapter we'll be digging a level deeper to show you exactly what's going on as we build and train our models. This means that you'll have learned the most important best practices used in modern deep learning—not just how to use them, but how they really work and are implemented.
	- fastai book code
		- Full and Stripped Notebooks: There are two folders containing different versions of the notebooks. The _full_ folder contains the exact notebooks used to create the book you're reading now, with all the prose and outputs. The _stripped_ version has the same headings and code cells, but all outputs and prose have been removed. After reading a section of the book, we recommend working through the stripped notebooks, with the book closed, and seeing if you can figure out what each cell will show before you execute it. Also try to recall what the code is demonstrating.


- todo
	- i need to set up my dell soon, so i can do cuda
		- or in the meantime, buy a sub to colab/paperspace if free colab or pytorch mac is not fast enough
	- if i do need cuda: there might not actually be much drawbacks despite windows keyboard?
		- bc i'll almost entirely be working in a jupyter notebook, and it seems most of the commands actually do not use modifier keys
			- bc there's edit and command mode, that is triggered by esc







## kaggle
[Is it a bird? Creating a model from your own data | Kaggle](https://www.kaggle.com/code/ahnjohn/is-it-a-bird-creating-a-model-from-your-own-data/edit)



# 2
- [Saving a basic fastai model | Kaggle](https://www.kaggle.com/code/ahnjohn/saving-a-basic-fastai-model/edit)

- study tips
	- quizzes
	- forums
- lots of tools
	- jupyter
		- use extensions
			- outline
			- collapsing headings
		- commands/shortcuts
		- can export as a py file (use the keyword)
	- gradio + huggingface
	- fastai repo
		- setup local stuff
		- setup website
	- github pages

# 3*
- (asterisk bc this lesson was awesome)


- 'clean' versions of the book chapters
- paperspace gradient is better than kaggle
	- nvm...they never have 'free gpus' available
- use jupyterlab
- functools.partial
- @interact
- mse
- gradient 
- pytorch tensors
- optimization
- gradient descent
- rectified linear
	- add as many as u want
	- higher dims 
- i'm confused why normalizing would do anything
	- shouldnt the relu's be able to handle any range, bc coeffs can be anything
	- also why wouldnt the process be able to realize that some inputs are just way larger, and just naturally make it so corresp weights be smaller, if they actually dont contribute that much?
	- though i do buy why log normalizing would be helpful...relu is linear, so if input is more 'linear', then maybe that helps with weights somehow
	- ohhh nvmm..proobly: larger number so larger deriv in that coord; so thinks it should mainly move in this dim
- huggingface transformers for nlp
- btw general idea he's explaining is like:
	- entirety of ML really is just: gradient descent
	- all this theory/research is just understanding how we can make this process better
	- btw the 'only' linalg you need is just: know how to do matrix multiplication...you don't need tons of semesters on it


# 4


- ulmfit, transformers
- finetuning 
- kaggle competitions are real world
- classification
- jupyter notebook tips
	- bash commands
		- can use python variables in them
- numpy, matplotlib, pandas, pytorch
	- pandas for tables
	- pytorch for dl
- df.describe 
- tokens
- transformers
- df.map
- ulmfit for larger docs
- separate training, validation, test sets
- overfit
- sklearn for basic/historical machine learning (good library)
- fastai wont let you train without validation set (which is not standard; but he thinks incredibly important)
	- often called 'test'
- to avoid overfit: pick a good validation set
	- do NOT pick randomly
- test set: to avoid overfitting on the validation set
- metric vs loss
	- you want loss to be smooth, so gradient is more useful
- the example is excellent
	- lots of little tips
	- plot a sample
	- alpha kw for scatter plots
- understand your outliers



# 5

- more jupyter notebook setup tips
- ?? instead of throwing away data: replace NaN with mode
	- how does this not make a difference???
	- well he's not saying it doesn't make a difference...he's just saying, in practice, this is good enough
- same concept comes up again, from prev lecture:
	- transform data so that the distribution in each dim looks more (pcwise) linear
		- aka usually use log
- tabular feature engineering
- apparently pytorch does everything np does, so no real need for much np in his workflow
	- ie matrix multiplication
	- pytorch better bc gpu and differentiation (aka more features), etc
- pytorch tensor = numpy array = math vector/matrix (and tensors for higher ranks??)
	- rank of a tensor = num dims
	- so a table = rank 2 tensor
- broadcasting: elementwise mult + sum is same as matrix mult, if second matrix is a vector
	- more efficient:
		- done in optimized c/cuda code
	- q: is this more efficient than if he had used the true 'matrix multiplication' operator from pytorch/numpy?
- harder to optimize if columns don't have same range
- pytorch: ending in underscore: in place op 
- sigmoid: easier to optimize
- sympy.plot
- python dynamic languge is why you can redefine inner function
	- aka benefit of exploratory in python
- use sigmoid if binary
- @ is official python op for mat multiply, tho python doesnt define an implementation for it
- prefer 'n x 1 matrix' over 'vector'
	- implement this with `,None`
	- trailing unit axis
- need to normalize, bc there needs to be a balance btwn 'loss size' and 'gradient size'
- framework
- why does ensembilng actually help?
	- even if diff random coeffs...why wouldnt they converge at the same place? are there multiple local mins?
- i feel like one important skill to review:
	- know all the common ways to plot data, esp for exploratory analysis
	- and understand which plots are typically used for which kinds of data
	- bc he uses a boxenplot for the fares, which seems really helpful
	- #/problems btw, have like actual 'problems' for this imo...have practice deducing what plots might work well..and also, what matplotlib/seaborn methods to use
		- these problems should be very easy to review, bc the steps are pretty simple:
			- step 1: determine structure of the data
			- step 2: recall which plots are applicable
			- step 3: recall the python methods
- #/fundy 
	- there's surely a few resources where i can find a lot of 'pro tips' about using jupyter notebooks + python libs for exploratory analysis
		- like that 'interact' feature...holy crap



# 6

- sklearn for classical (non deep learning) machine learning methods
- why is building model from random subsets...why is that uncorrelated?
	- aka subsets are still overlapping..isnt that correlation?
- idea: random models via: same model, random inputs
- tabular data
	- random forest
	- feature importance
-  
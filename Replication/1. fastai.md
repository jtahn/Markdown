# refs
- https://course.fast.ai/
- [Practical Deep Learning for Coders - YouTube](https://www.youtube.com/playlist?list=PLfYUBJiXbdtSvpQjSnJJ_PmDQB_VyT5iU)
- [Practical Deep Learning for Coders - The book](https://course.fast.ai/Resources/book.html#nbviewer)

# 1
- [Practical Deep Learning for Coders - 1: Getting started](https://course.fast.ai/Lessons/lesson1.html)
- [Practical Deep Learning for Coders: Lesson 1 - YouTube](https://www.youtube.com/watch?v=8SF_h3xF3cE&list=PLfYUBJiXbdtSvpQjSnJJ_PmDQB_VyT5iU&index=1)
- [Jupyter Notebook Viewer](https://nbviewer.org/github/fastai/fastbook/blob/master/01_intro.ipynb#)
- [Is it a bird? Creating a model from your own data | Kaggle](https://www.kaggle.com/code/ahnjohn/is-it-a-bird-creating-a-model-from-your-own-data/edit)
- https://course.fast.ai/Lessons/Summaries/lesson1.html


- 


# 2
- [Practical Deep Learning for Coders - 2: Deployment](https://course.fast.ai/Lessons/lesson2.html)
- [Lesson 2: Practical Deep Learning for Coders 2022 - YouTube](https://www.youtube.com/watch?v=F4tvM4Vb3A0&list=PLfYUBJiXbdtSvpQjSnJJ_PmDQB_VyT5iU&index=2)
- [Jupyter Notebook Viewer](https://nbviewer.org/github/fastai/fastbook/blob/master/02_production.ipynb)
- [Saving a basic fastai model | Kaggle](https://www.kaggle.com/code/ahnjohn/saving-a-basic-fastai-model/edit)
- https://course.fast.ai/Lessons/Summaries/lesson2.html


- study tips
	- quizzes
	- forums
- lots of tools
	- jupyter
		- use extensions
			- outline
			- collapsing headings
		- commands/shortcuts
		- can export as a py file (use the keyword)
	- gradio + huggingface
	- fastai repo
		- setup local stuff
		- setup website
	- github pages

# 3*
- (asterisk bc this lesson was awesome)

- [Practical Deep Learning for Coders - 3: Neural net foundations](https://course.fast.ai/Lessons/lesson3.html)
- [Lesson 3: Practical Deep Learning for Coders 2022 - YouTube](https://www.youtube.com/watch?v=hBBOjCiFcuo&list=PLfYUBJiXbdtSvpQjSnJJ_PmDQB_VyT5iU&index=3)
- [Jupyter Notebook Viewer](https://nbviewer.org/github/fastai/fastbook/blob/master/03_ethics.ipynb)
- https://course.fast.ai/Lessons/Summaries/lesson3.html


- 'clean' versions of the book chapters
- paperspace gradient is better than kaggle
	- nvm...they never have 'free gpus' available
- use jupyterlab
- functools.partial
- @interact
- mse
- gradient 
- pytorch tensors
- optimization
- gradient descent
- rectified linear
	- add as many as u want
	- higher dims 
- i'm confused why normalizing would do anything
	- shouldnt the relu's be able to handle any range, bc coeffs can be anything
	- also why wouldnt the process be able to realize that some inputs are just way larger, and just naturally make it so corresp weights be smaller, if they actually dont contribute that much?
	- though i do buy why log normalizing would be helpful...relu is linear, so if input is more 'linear', then maybe that helps with weights somehow
	- ohhh nvmm..proobly: larger number so larger deriv in that coord; so thinks it should mainly move in this dim
- huggingface transformers for nlp
- btw general idea he's explaining is like:
	- entirety of ML really is just: gradient descent
	- all this theory/research is just understanding how we can make this process better
	- btw the 'only' linalg you need is just: know how to do matrix multiplication...you don't need tons of semesters on it


# 4


- ulmfit, transformers
- finetuning 
- kaggle competitions are real world
- classification
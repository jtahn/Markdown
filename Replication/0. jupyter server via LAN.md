# replication goals and requirements
- 'accessing' (aka i can type and run stuff) jupyterlab on my macbook/chromebook/ipad, but it is actually 'running' everything on a windows computer with nvidia/cuda gpu
- only need to 'serve' 1 remote user (aka me)
- LAN (aka both host/server and 'remote' users are on the same home wifi)

---

what this is NOT: 'better than online providers'
- everyone says to use online services (colab, paperspace, kaggle) if you're just trying to learn some intro ML
- probly bc the paid sub (like 10usd/month) wont be that expensive...bc you don't use that much resources when you're learning?
	- aka probly would spend more money if you bought your own setup just for intro ML
- also kaggle actually beat this setup:

don't do it cuz it's faster; cuz my p5200 was actually 2 seconds slower than kaggle, at least for the first example in the fastai book (on pets). my 2 epochs were 27 and 35s, vs 25 and 33s for kaggle (gpu t4 x2)

that's the card that on paper seems to have like identical specs to p5200, except it has the 320 tensor cores (i'm guessing that's a big deal).

https://www.techpowerup.com/gpu-specs/tesla-t4.c3316



# why this replication/writeup
(aka why this process is worth doing + saving details)

- for me it seemed worth it bc i have 2 extra laptops...p5200 and gtx 2070 mobile
	- https://www.techpowerup.com/gpu-specs/geforce-rtx-2070-mobile.c3349
- so this exact entire setup will be useful to me in the future
	- i'll need to run this setup again, for my lenovo (rtx2070 mobile) 
		- leave one of these laptops with gregs, and the other at brown/damini/etc
- also possibly useful others
	- i'm sure there are lots of other ppl like me that:
		- awhile ago, got a windows pc bc also wanted to game
		- now have a mac (or a more 'productivity focused' windows laptop that doesn't have (as strong) a gpu) and want to study ML



- i'm learning a ton of concepts that are surely important to be aware of as a swe
	- i'll have to know these eventually, so might as well figure it out now
	- i knew literally nothing about all of networking/shell/linux/ml/ai/vm; and i was barely vaguely familiar with python environments
	- consider this like a project that implements concepts from https://missing.csail.mit.edu/
		- this whole steup likely would've been far easier if i went through this first....
			- literally knew none of the jargon/concepts, barely knew what i should be googling

- the details of this full setup, is difficult to figure out if you're someone like me
	- if ur interested in ML..i'm guessing you probly alrdy know how to code somewhat
		- but i bet there's a decent amt of those ppl who (like me) only really exposed to programming within an academia context
		- aka just installed conda+jupyter into base environment, and that was basically the set up
	- issues with the resources for ppl like me:
		- no one is really explaining why they do stuff, what the commands do, etc
		- so there's a lot of distracting/extra stuff that you actually don't need, but it's to figure this out
			- ie don't need ssh
		- not many resources that do this entire setup
			- so you need to frankenstein tons of scattered stuff
		- bc of how fast the software tools are changing/updating, often the stuff they're doing is some combo of:
			- deprecated
			- not best practice








# after setup
aka to start server on host, and then connect on remote:
(btw, server runs as long as pc doesnt turn off, so you can close window on remote, and just reopen later to resume)


- if WSL host:
	- in conda on host: jupyter lab --ip 0.0.0.0 --port 8888 --no-browser
	- in browser on remote: (windows-host-ip):8889
- if mac host:
	- in conda on host: jupyter lab --ip (host-ip) --port 8888 --no-browser
	- in browser on remote: (host-ip):8888


#/todo 
to find ip:
wsl:
mac:


# full setup
(starting from fresh install, of a windows pc as the host/server)

## windows

first, we create a local user.
(i don't need to sign in with microsoft account; also seems like a possible security risk since i don't fully understand the networking stuff that i do later)

- iirc need to input some bios commands on startup


then in windows settings:
- system > power and battery > "when plugged in, put my device to sleep" NEVER


now install manufacturer app that allows you to set battery limiter. 
- set max battery charge to anywhere 50-80pct
	- bc will always be plugged in, and iirc otherwise battery will swell

- dell is:
- lenovo is:
#/todo

if not alrdy installed: windows terminal:


## WSL
(directly from https://learn.microsoft.com/en-us/windows/wsl/setup/environment)

in windows command prompt:
```bash
wsl --install
```

restart computer. after restart, a windows terminal popup will prompt you to create an ubuntu account, so enter ur desired user/pwd.

then in 'ubuntu':
```bash
sudo apt update && sudo apt upgrade
```



for convenience:
first, pin 'ubuntu' app to windows task bar. you can use this app to open ur wsl/linux/distro's shell
then, in 'ubuntu': open your wsl's home dir in windows explroer:
```bash
explorer.exe .
```
Pin the directory that appears (to explorer's quick access), since i'm guessing it'll be nice to be able to navigate here easily if i ever need to in windows



## cuda
(directly from https://docs.nvidia.com/cuda/wsl-user-guide/index.html#getting-started-with-cuda-on-wsl-2)

in windows, dl + install driver corresp to your windows + gpu, at https://www.nvidia.com/Download/index.aspx

in 'ubuntu', remove GPG key:
```bash
sudo apt-key del 7fa2af80
```
 
 in 'ubuntu', run installation code provided by https://developer.nvidia.com/cuda-downloads
 ie [[../!assets/attachments/Pasted image 20240611002451.png]]
```bash
wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin
sudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/12.5.0/local_installers/cuda-repo-wsl-ubuntu-12-5-local_12.5.0-1_amd64.deb
sudo dpkg -i cuda-repo-wsl-ubuntu-12-5-local_12.5.0-1_amd64.deb
sudo cp /var/cuda-repo-wsl-ubuntu-12-5-local/cuda-*-keyring.gpg /usr/share/keyrings/sudo apt-get update
sudo apt-get -y install cuda-toolkit-12-5
```


!!! warning: many 'guides' change the last line to
```bash
sudo apt-get -y install cuda
```

DO NOT DO THIS. as nvidia guide says:

> Once a Windows NVIDIA GPU driver is installed on the system, CUDA becomes available within WSL 2. The CUDA driver installed on Windows host will be stubbed inside the WSL 2 as `libcuda.so`, therefore **users must not install any NVIDIA GPU Linux driver within WSL 2**. One has to be very careful here as the default CUDA Toolkit comes packaged with a driver, and it is easy to overwrite the WSL 2 NVIDIA driver with the default installation. We recommend developers to use a separate CUDA Toolkit for WSL 2 (Ubuntu) available from the [CUDA Toolkit Downloads](https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=WSL-Ubuntu&target_version=2.0) page to avoid this overwriting. This WSL-Ubuntu CUDA toolkit installer will not overwrite the NVIDIA driver that was already mapped into the WSL 2 environment.
> 
> The installation instructions for the CUDA Toolkit can be found in the CUDA Toolkit download page for each installer. But DO NOT choose the “`cuda`”, “`cuda-12-x`”, or “`cuda-drivers`” meta-packages under WSL 2 as these packages will result in an attempt to install the Linux NVIDIA driver under WSL 2.

you can look at https://docs.nvidia.com/cuda/cuda-installation-guide-linux/#meta-packages for more info about the meta packages

i personally did
```bash
sudo apt-get -y install cuda-toolkit
```
since i (naively) thought auto-updates would be preferable...but now i'm guessing maybe not...so i'll just stick with the og directions above, when i run this setup again


to verify that everything worked: in 'ubuntu':
```bash
nvidia-smi
```

this runs the https://developer.nvidia.com/system-management-interface
and you should see details about your nvidia gpu

- anecdote/troubleshooting:
	- i got 'segmentation fault' the first few times i tried running this
		- even when i closed and re-opened 'ubuntu'
	- this seemed to solve it (idk why)
		- first run `nvidia-smi` in the windows terminal
		- now `nvidia-smi` should work in 'ubuntu'


## conda
(directly from https://github.com/conda-forge/miniforge?tab=readme-ov-file#unix-like-platforms-mac-os--linux)

in 'ubuntu':
```bash
wget "https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh"
bash Miniforge3-$(uname)-$(uname -m).sh
```
ie [[../!assets/attachments/Pasted image 20240611010935.png]]
(i chose the wget variant bc thats what the nvidia guide used above)

the shell will ask you some question: i just did defaults:
- install in the suggested directory (should be the wsl home directory you pinned earlier)
- at the end, they ask "Do you wish to update your shell profile"
	- say yes!
		- because: #/todo


then 'reload' the 'ubuntu' shell after installation. you could just close and re-open 'ubuntu', or enter any of:
```bash
. ~/.bashrc
source ~/.bashrc
exec bash
```
(see https://stackoverflow.com/a/35073891)

to verify everything went well, in 'ubuntu', enter either of:
```bash
conda --version
which python
```

- why is `which python` 'equivalent'? like what if ur wsl/linux distro already came with python...how would you know whether you're looking at linux's python vs conda's python?


---

why miniforge?
- #/todo


what is the meaning of the miniforge installation code?
- wget is a program that dls a file from the web
	- https://en.wikipedia.org/wiki/Wget
		- computer program that retrieves content from web servers
	- in this case, the installer/executable
- but how do we get the installer that corresp to our system?
	- the code essentially is like 'string formatting'
- the `$(command)` syntax is command substitution
	- https://en.wikipedia.org/wiki/Command_substitution
	- https://www.gnu.org/software/bash/manual/html_node/Command-Substitution.html
	- https://ss64.com/bash/syntax-substitution.html
- here the command is `uname`
	- https://ss64.com/bash/uname.html
		- `-m  --machine      Print the machine (hardware) type.`
		- `-s  --kernel-name     Print the kernel name.`
			- If no options are given, 'uname' acts as if the '-s' option were given.
- the `bash` command runs the file that was just dl by `wget`
	- https://ss64.com/bash/bash.html


## graphviz
(from https://www.graphviz.org/download/#linux)
in 'ubuntu':
```bash
sudo apt install graphviz
```

---

this step is necessary if you plan to use graphviz.
in particular, the fastai course/book uses it to run jupyter cells that display diagrams, like 
```python
gv('''program[shape=box3d width=1 height=0.7]
inputs->program->results''')
```
without this step, you will get an error message like
```
...make sure the Graphviz executables are on your systems' PATH.'
```


this is because the `graphviz` python package is just an 'interface' to the 'actual' graphviz 'library/software'
- https://graphviz.readthedocs.io/en/stable/manual.html
	- "The only dependency is a working installation of [Graphviz](https://www.graphviz.org/) ([download page](https://www.graphviz.org/download/), [archived versions](https://www2.graphviz.org/Archive/stable/), [installation procedure for Windows](https://forum.graphviz.org/t/new-simplified-installation-procedure-on-windows/224))."




## conda env
we will create a conda env, and install the required python packages inside it. namely pytorch and fastai.


first, in 'ubuntu':
```bash
conda 
```

- does conda env creation default to the most recent version of python?

now activate env:
```bash
conda activate myenv
```


in 'ubuntu', follow https://pytorch.org/get-started/locally/
ie [[../!assets/attachments/Pasted image 20240611013354.png]]
```bash
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
```

in 'ubuntu', follow https://docs.fast.ai/
```bash
conda install -c fastai fastai
```


now verify that at least pytorch is working:
in 'ubuntu', start python interpreter:
```bash
python
```
then in python prompt:
```python
Python 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import torch
>>> torch.cuda.is_available()
True
>>> torch.cuda.get_device_name(0)
'Quadro P5200'
```

(you should see ur nvidia gpu)
send `ctrl` + `d` to exit python prompt


now install other python packages: in 'ubuntu':
```bash
conda install notebook, fastbook
```

why these packages:
- notebook
	- installs jupyterlab
- fastbook
	- if ur studying this
- jupyterlab extensions
	- go look for the rec'd ones, myabe:
		- spellchecker, code-formatter


---


#/todo
- anecdote/troubleshooting
	- remember to activate conda env before you install packages!
		- if you forget, i think nbd...just clone the env; i'm sure it's nbd that base still has pytorch in it
	- if conda not recognized:



## network
variable explanations: that you should replace stuff with:
`{WIN_IP}` is the local ip of the windows machine
(wsl-ip) is the 'local ip' of the wsl distro
(win-lh) refers to the 'localhost  ip' of windows machine
(wsl-lh) same but for wsl distro...tho i'm pretty sure this actually is 'same thing' as (win-lh), bc of how windows implements wsl networking

---


in windows, run cmd prompt in admin:
```bash
netsh advfirewall firewall add rule name="Jupyter LAN" dir=in action=allow protocol=TCP localport=8889 
netsh interface portproxy add v4tov4 listenport=8889 listenaddress={WIN_IP} connectport=8888 connectaddress=localhost
```

in 'ubuntu':
```bash
jupyter notebook password
```
then set a password
(you will sometimes need to enter this, when you connect)

---
you can replace 8888 and 8889 with whatever ports you want, but these are just 'best practice' for jupyter (jup defaults to port 8888)

- wait..acutally i think i can replace 8889 with 8888?
	- wouldnt be infinite loop bc localhost and local-ip are diff?
	- tho lowkey...maybe this is not best practice, and would be confusing to ppl?


---

- this is a great vid
	- [WSL 2 Networking - YouTube](https://www.youtube.com/watch?v=yCK3easuYm4)
	- wsl ip is only visible to the windows host
	- port proxy via netsh
	- need to open firewall port for the app
	- !!!!there is a gui in windows that lets you browse/edit your firewall rules easily




### todo, massive cleanup

https://stackoverflow.com/questions/64513964/wsl-2-which-ports-are-automatically-forwarded

---

- localhost and local ip are not the same thing. if jup listening on local ip, then navigating to localhost cannot see it. so localhost isn't actually stricter. they're completely different things - Search
- forwarding lets you use it, UNLESS it is localhost - Search
- what does it mean to listen on 0.0.0.0 - Search
- i think 0.0.0.0 essentially means that both localhost and local ip will let you see AND enter the website. aka can think of 'listening' here as like 'where is website hosted', and it's basically like multiple places? - Search
- so this is what you want to set jupyter to; otherwise if you do local ip (via python command), it's annoying bc then you need to update the netsh portproxy every time you restart - Search
- this also makes me think that: then portproxy: can just be listening on local-ip; this wasn't the problem; yes it wasn't. idk wtf i did wrong lol - Search




summary: a couple things are happening
go confirm my understanding on these (i really doubt this is actually that difficult...ppl would've designed these network stuff in a way that makes sense)..but from rough googling and just trying a bunch of things, here's what seems to be happening:


an 'app' will ALWAYS be hosted at: 'host ip'+ 'specified (listen) port'.
like from any device: if you're allowed to reach this app, then this is the 'website' where you reach the app.
actually...no...'host ip' could be either 'local host' or 'local ip'...?


now the question is, who is allowed to actually connect to the website.
and i think this is what the 'listen address' does.
if you set it to 0.0.0.0, then it allows anyone on lan network to connect to the website.
(which is what i want)
so this is why, the netsh: you need to set listenaddress to 0.0.0.0.

for the port, i think: it depends on whether it needs to be a different port as the connect port. but just to be safe, might as well.
basically, i'm pretty sure it can be same port, if you set jupyterlab to run explicitly on the wsl's ethernet ip.
bc if you do that, then on windows, you actually can't access the app from localhost:8888. you have to specifically navigate to (wsl ip):8888.
so then i don't think there'd be any problems...
but if you set jupyter ip to 'listen' on ip's in a way that results in:
windows being able to connect via localhost:8888
then i suspect, you do need different port.
otherwise you're just going to portproxy to itself?
well actually wait...maybe you're expanding the list of addresses that can connect?



but also: the jupyter on wsl: you need to set it to ip that is either the actual wsl ethernet, or the 0.0.0.0
for some reason, setting no ip does not work, and i'm not sure why.
basically, when you portproxy:
then remote will get '403 : Forbidden'.
as if: not allowed to connect

on the windows: localhost:8888 AND localhost:8889 can connect
(windows-ip):8888 doesn't even connect
but (windows-ip):8889 gets forbidden


!!!!! oh ok this scenario is key to understanding what's going on imo
there's something special about how wsl and windows are connected, esp via localhost

also means that, there's actually 2 levels:
- you can connect/see the website
- you are allowed to view content

- and what portproxy does it:
	- does not change the 'identity' of who is connecting
	- it just basically provides another 'website' that ppl can navigate to
	- but note that localhost will not let other ppl access it

entering local ip in address bar, will go to router, and then come back


- if you don't put an ip address for jupyter server, then it defaults to localhost
	- this means that:
		- website is hosted at localhost:8888
		- only localhost can see to website
		- only localhost can enter the website
	- and thus this means that, portproxy:
		- needs to connect to localhost, to let others see the website via (precision-ip):8889
			- btw: just say that (precision-ip) is the local ip of windows, so i can just pretend this is like a variable
		- however, this is basically pointless: portproxy does not change identity, so no one else other than localhost is allowed to enter

- if you put (wsl-ip) for jupyter server, then:
	- website is hosted at (wsl-ip):8888
	- precision can navigate, via (wsl-ip):8888
	- so to fwd, then portproxy needs to connect to (wsl-ip):8888
- 



- one key confusion i have rn is: i'm pretty sure jupyterlab, there were times where i started it with


- importantly, localhost and local ip are NOT the same thing


- btw, localhost is stricter than local-ip
	- if you only listen on localhost, then local-ip cannot see it


- the final thing i'm confused about though.
	- i'm fairly certain last night, i had portproxy only listening on the windows-ip, and it was working
	- similarly, i'm fairly certain that when i ran notebook 'servers' on mac and windows, i only had to do the local-ip for the command line option, and i could access remotely...
	- aka so why all of a sudden, i need to set things to 0.0.0.0?
	- maybe i'm just remembering wrong tbh....or better: this is probly obvi af once i actually understand what 'listening' means



- the key thing is that wsl allows the windows host machine to also be interpreted as localhost
	- so there's like '2 different ways' to connect:
		- you can do the 'private ethernet connection' btwn them, that has like an actual ip assigned to wsl
		- or use the feature where wsl sees windows as localhost as well





also the wsl address: the only thing that can connect to it is windows. its like the windows and wsl are connected by ethernet.
which means on a different computer on lan: the way to connect to wsl app, is via:
use the portproxy to connect wsl app to a windows app.
so then on my local/remote computer on lan network, the app is now available via windows address + windows port






---

- https://learn.microsoft.com/en-us/windows/wsl/networking#connecting-via-remote-ip-addresses
	- i think the 'Connecting via remote IP addresses' is more relevant to my use case, then the 'Accessing a WSL 2 distribution from your local area network (LAN)' heading
		- the latter seems like, being able to basically use the linux prompt...which isn't what i need
		- i just want to be able to access the jupyter server
	- In this example, you will need to update `<yourPortToForward>` to a port number, for example `listenport=4000`. `listenaddress=0.0.0.0` means that incoming requests will be accepted from ANY IP address. The Listen Address specifies the IPv4 address for which to listen and can be changed to values that include: IP address, computer NetBIOS name, or computer DNS name. If an address isn't specified, the default is the local computer. You need to update the `<yourPortToConnectToInWSL>` value to a port number where you want WSL to connect, for example `connectport=4000`. Lastly, the `connectaddress` value needs to be the IP address of your Linux distribution installed via WSL 2 (the WSL 2 VM address), which can be found by entering the command: `wsl.exe hostname -I`.
	- Using a lowercase "i" with the hostname command will generate a different result than using an uppercase "I". `wsl hostname -i` is your local machine (127.0.1.1 is a placeholder diagnostic address), whereas `wsl hostname -I` will return your local machine's IP address as seen by other machines and should be used to identify the `connectaddress` of your Linux distribution running via WSL 2.

- if i want to pull up jupyterlab on windows, 



![[../!assets/attachments/Pasted image 20240611012146.png]]



---


- https://learn.microsoft.com/en-us/windows/wsl/networking#accessing-a-wsl-2-distribution-from-your-local-area-network-lan

WSL 2. WSL 2 has a virtualized ethernet adapter with its own unique IP address. Currently, to enable this workflow you will need to go through the same steps as you would for a regular virtual machine. (We are looking into ways to improve this experience.)

Here's an example of using the [Netsh interface portproxy](https://learn.microsoft.com/en-us/windows-server/networking/technologies/netsh/netsh-interface-portproxy) Windows command to add a port proxy that listens on your host port and connects that port proxy to the IP address for the WSL 2 VM.

astly, the `connectaddress` value needs to be the IP address of your Linux distribution installed via WSL 2 (the WSL 2 VM address), which can be found by entering the command: `wsl.exe hostname -I`.


btw it seems: the 'listenport' means: the port that other devices will enter.
basically, the app 'listens' for commands/actions at that port.
so that's why, device goes to that port to put the commands.
aka we put that in browser.


---
then on windows, open terminal/etc as admin:

```
netsh interface portproxy add v4tov4 listenport=8888 listenaddress=(windows-ip) connectport=8888 connectaddress=localhost
```



---

- https://gist.github.com/wllmsash/1636b86eed45e4024fb9b7ecd25378ce
	- WSL2 uses Hyper-V for networking. The WSL2 network settings are ephemeral and configured on demand when any WSL2 instance is first started in a Windows session. The configuration is reset on each Windows restart and the IP addresses change each time. The Windows host creates a hidden switch named "WSL" and a network adapter named "WSL" (appears as "vEthernet (WSL)" in the "Network Connections" panel). The Ubuntu instance creates a corresponding network interface named "eth0".
- !!!! https://askubuntu.com/questions/1403508/access-a-web-application-running-in-ubuntu-wsl-from-another-pc-on-the-network/1403545#1403545



You may also need a firewall rule, as mentioned in the comments. However, you _should_ typically receive a request from Windows Defender Firewall to allow the connection the first time you use it.

---

ou show your `/etc/ssh/sshd_config` with the SSH server running on the default port of 22. If this is the case, then you will need to adjust your port forwarding `connectport` to:

```
netsh interface portproxy add v4tov4 listenport=4000 listenaddress=0.0.0.0 connectport=22 connectaddress=192.168.101.100

```

This will forward connections from port 4000 on the Windows host to port 22 on the WSL2 host.


---

the following is not enough; i think bc of what ppl say above: basically the way the wsl ip works: only the windows host machine can access/see it.
so this is why you need to make some kind of connection btwn windows and wsl machine.




```
jupyterlab --NotebookApp.token='' --NotebookApp.password='' \
 --ip $(python3 -c "import subprocess; subprocess>
```

you will get an output like this
```
[I 2022-05-17 12:25:06.476 ServerApp] nbclassic | extension was successfully loaded.
[I 2022-05-17 12:25:06.476 ServerApp] Serving notebooks from local directory: /mnt/d
[I 2022-05-17 12:25:06.476 ServerApp] Jupyter Server 1.4.1 is running at:
[I 2022-05-17 12:25:06.476 ServerApp] http://172.28.20.187:8888/lab
[I 2022-05-17 12:25:06.476 ServerApp]  or http://127.0.0.1:8888/lab
[I 2022-05-17 12:25:06.476 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
```

So 172.28.20.187:8888 is the IP and the Port used by Jupyter which changes every time. 









# move above

## why wsl
- https://learn.microsoft.com/en-us/windows/wsl/about
	- WSL 2 is the default distro type when installing a Linux distribution. WSL 2 uses virtualization technology to run a Linux kernel inside of a lightweight utility virtual machine (VM). Linux distributions run as isolated containers inside of the WSL 2 managed VM.
	- wsl 2 is the default when you install
- https://en.wikipedia.org/wiki/Windows_Subsystem_for_Linux
- https://github.com/microsoft/wslg?tab=readme-ov-file#install-instructions-fresh-install---no-prior-wsl-installation
	- wslg already comes with fresh installs of wsl
- https://developer.nvidia.com/cuda/wsl
	- Microsoft Windows is a ubiquitous platform for enterprise, business, and personal computing systems. However, industry AI tools, models, frameworks, and libraries are predominantly available on Linux OS. Now all users of AI - whether they are experienced professionals, or students and beginners just getting started - can benefit from innovative GPU-accelerated infrastructure, software, and container support on Windows.
		- basically, linux is king for AI dev
		- that's why we need WSL
- https://pytorch.org/blog/microsoft-becomes-maintainer-of-the-windows-version-of-pytorch/
	- due to limited resources, Windows support for PyTorch has lagged behind other platforms. Lack of test coverage resulted in unexpected issues popping up every now and then. Some of the core tutorials, meant for new users to learn and adopt PyTorch, would fail to run. The installation experience was also not as smooth, with the lack of official PyPI support for PyTorch on Windows. Lastly, some of the PyTorch functionality was simply not available on the Windows platform, such as the TorchAudio domain library and distributed training support. To help alleviate this pain, Microsoft is happy to bring its Windows expertise to the table and bring PyTorch on Windows to its best possible self.
		- too difficult to maintain pytorch for windows
- https://docs.fast.ai/#windows-support
	- We recommend using Windows Subsystem for Linux (WSL) instead – if you do that, you can use the regular Linux installation approach, and you won’t have any issues with `num_workers`.
		- there are some issues if you use windows directly







# future


## possibly reliable setup tutorials
- https://github.com/fastai/fastsetup/blob/master/README.md
- jeremy howard live coding session https://www.youtube.com/watch?v=56sIyFjihEc&list=PLfYUBJiXbdtSLBPJ1GMx-sQWf6iNhb8mM
	- https://forums.fast.ai/t/live-coding-aka-walk-thrus/96617

- https://ilyasergey.net/YSC2229/wsl.html


## address skipped stuff mentioned by the microsoft guide

- windows terminal
	- for some reason, i alrdy had this installed and it alrdy seemed to be default for the 'ubuntu' app
- vscode setup for now, bc using jupyter
- git bc i'm just using this for fastai course...so i'm not really writing my own code rn
- docker
	- not necessary atm, see more info below
- database
- gpu acceleration
	- see nvidia directions in next step


## install jupyterlab in base?
- if i end up using tons of environments

- [anaconda - Best practice for Jupyter to be installed in the Conda base environment or separately in each new environment? - Stack Overflow](https://stackoverflow.com/questions/76877034/best-practice-for-jupyter-to-be-installed-in-the-conda-base-environment-or-separ)
- [How to set up Anaconda and Jupyter Notebook the right way | by Justin Güse | Towards Data Science](https://towardsdatascience.com/how-to-set-up-anaconda-and-jupyter-notebook-the-right-way-de3b7623ea4a)
- [Bits of Analytics - Using different conda environments with Jupyter Lab](https://bitsofanalytics.org/posts/jupyter-conda-envs/jupyter_conda_envs#run-jupyter-lab-from-base-environment)

In the base environment, run `pip install jupyterlab`.

Installing Jupyter Lab and its extensions inside the base environment will make it available to all future conda environments and won't require separate setups.



When you launch Jupyter Lab, you will see that none of your conda environments will be available in the Launcher. To add existing (or new) conda envs to Jupyter Lab, use the following commands:

```bash
# Create a new env
$ conda create -n new_env -y

# Install ipykernel and add the env
$ pip install ipykernel
$ ipython kernel install --user --name=new_env
```

Once you restart your Jupyter Lab session, the env will be available in the Launcher.



## docker containers

- https://learn.microsoft.com/en-us/windows/ai/directml/gpu-cuda-in-wsl
	- Now follow the instructions in the [NVIDIA CUDA on WSL User Guide](https://docs.nvidia.com/cuda/wsl-user-guide/index.html#getting-started-with-cuda-on-wsl-2) and you can start using your exisiting Linux workflows through [NVIDIA Docker](https://github.com/NVIDIA/nvidia-docker), or by installing [PyTorch](https://pytorch.org/get-started/locally/) or [TensorFlow](https://www.tensorflow.org/install/gpu) inside WSL.
		- aka cuda can be used/accessed either via docker containers, or 'directly' with pytorch
- https://docs.nvidia.com/deeplearning/frameworks/user-guide/index.html#runcont
	- Over the last few years there has been a dramatic rise in the use of software containers for simplifying deployment of data center applications at scale. Containers encapsulate an application along with its libraries and other dependencies to provide reproducible and reliable execution of applications and services without the overhead of a full virtual machine.
	- The NVIDIA Container Runtime for Docker, also known as [nvidia-docker2](https://github.com/NVIDIA/nvidia-docker) enables GPU-based applications that are portable across multiple machines, in a similar way to how Docker® enables CPU-based applications to be deployed across multiple machines.
	- summary
		- aka docker/container isn't necessary for me rn for learning/exploring, but seems mandatory for industry
		- ie stuff like this seems useful
			- https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch
				- The PyTorch NGC Container comes with all dependencies included, providing an easy place to start developing common applications, such as conversational AI, natural language processing (NLP), recommenders, and computer vision.
				- The PyTorch NGC Container is optimized for GPU acceleration, and contains a validated set of libraries that enable and optimize GPU performance. This container also contains software for accelerating ETL ([DALI](https://developer.nvidia.com/dali), [RAPIDS](https://rapids.ai/)), Training ([cuDNN](https://developer.nvidia.com/cudnn), [NCCL](https://developer.nvidia.com/nccl)), and Inference ([TensorRT](https://nvidia.github.io/Torch-TensorRT/)) workloads.
- https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/index.html
	- The NVIDIA Container Toolkit enables users to build and run GPU-accelerated containers.
		- aka i don't need it rn, bc not using containers
	- same as https://docs.nvidia.com/ai-enterprise/deployment-guide-vmware/0.1.0/docker.html




## vscode

- [Remote Development - Visual Studio Marketplace](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.vscode-remote-extensionpack)
- [Work in Windows Subsystem for Linux with Visual Studio Code](https://code.visualstudio.com/docs/remote/wsl-tutorial#_run-in-wsl)


## setup mac as host
- this was absurdly easy: only need to do the conda env setup
- but idk when i would ever need to do this...i'd just code locally on my macbook


on mac, to get the ip, you do 'ipconfig getifaddr en0'...but i should never need this, i wouldnt host server on mac...unless ig i want to practice metal, and then somehow coding on a different mac? i guess if i buy a mac studio, then this could come in handy. also on mac, just same thing: go to wifi network settings and check more details about the connected network, you'll see the local ip there as well


## any future modifications
- if they only require commands sent through 'ubuntu', then this can actually be done remotely through jupyterlab
	- open up a terminal instance/kernel
	- cd ~ to get to home directory
		- (not actually sure if this is needed when you do the 'sudo apt' installation stuff?)
		- https://en.wikipedia.org/wiki/Cd_(command)#Usage
		- https://en.wikipedia.org/wiki/Home_directory#Unix
	- run the sudo commands


## ssh for access when away from home
- the issue is i don't know anything about security concerns


## possibly useful refs/concepts to add
- https://superuser.com/questions/349481/command-prompt-in-windows-and-linux-what-is-their-language-called
	- in both Linux and Windows, the great majority of "commands" that do useful things are in fact external programs that are "called" by the the shell. And that is the primary purpose of a shell, to enable an operator to start programs with specifying arguments for those programs. It's not really strictly a programming language, it's just a framework for launching programs.

## is this setup still useful for serious pros?

- maybe 'owning your own local hardware for personal experimenting' becomes worth it when you're doing heavy stuff? bc then the online sub cost is high?
	- so then this setup process is p nice imo, bc means i can literally just buy any prebuilt gaming pc with nvidia gp, run this setup, and i'm ready to go
	- prebuilt bc iirc, bapc isnt actually cheaper
	- gaming bc: by far the best value. but otoh, seems recently nvidia is purposely limiting the memory bandwitch (since gamers don't rly need huge amts, but it's absurdly important for ML) so that ppl either pay absurd amts for the 'data center' cards, or so that ppl use the gpu cloud service providers (who then buy those data center cards)
		- seems datacenters/etc are forced to buy the incredibly expensive 'datacenter' cards bc they need the 'guaranteed accuracy'
		- but for personal exploration, i likely do not need this; so gaming cards are find


and despite the gimping, gaming cards are still way better value




## dealing with zone.identifier files
- the Zone.Identifier files are just something windows generates when you move stuff from windows file system to WSL?
	- https://github.com/microsoft/WSL/issues/7456
	- apparently it's fine to just delete them
	- at some point, this should be fixed
	- 

[LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)

# Abstract
We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.

# refs
- touvron2023 https://arxiv.org/pdf/2302.13971v1
- https://paperswithcode.com/method/llama
- https://en.wikipedia.org/wiki/Llama_(language_model)
- https://simonwillison.net/2023/Aug/9/llama-from-scratch/
	- https://blog.briankitano.com/llama-from-scratch/
	- https://github.com/bkitano/llama-from-scratch
- https://github.com/openlm-research/open_llama
- https://news.ycombinator.com/item?id=40408880
	- https://github.com/naklecha/llama3-from-scratch
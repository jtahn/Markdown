[310. Minimum Height Trees](https://leetcode.com/problems/minimum-height-trees/)

```python
class Solution:
    def findMinHeightTrees(self, n: int, edges: List[List[int]]) -> List[int]:
        
```

# Description

A tree is an undirected graph in which any two vertices are connected by _exactly_ one path. In other words, any connected graph without simple cycles is a tree.

Given a tree of `n` nodes labelled from `0` to `n - 1`, and an array of `n - 1` `edges` where `edges[i] = [a_i, b_i]` indicates that there is an undirected edge between the two nodes `a_i` and `b_i` in the tree, you can choose any node of the tree as the root. When you select a node `x` as the root, the result tree has height `h`. Among all possible rooted trees, those with minimum height (i.e. `min(h)`)  are called **minimum height trees** (MHTs).

Return _a list of all **MHTs'** root labels_. You can return the answer in **any order**.

The **height** of a rooted tree is the number of edges on the longest downward path between the root and a leaf.

**Example 1:**  
![](!assets/attachments/Pasted%20image%2020240420003556.png)  
**Input:** `n = 4, edges = [[1,0],[1,2],[1,3]]`  
**Output:** `[1]`  
**Explanation:** As shown, the height of the tree is 1 when the root is the node with label 1 which is the only MHT.

**Example 2:**  
![](!assets/attachments/Pasted%20image%2020240420003607.png)  
**Input:** `n = 6, edges = [[3,0],[3,1],[3,2],[3,4],[5,4]]`  
**Output:** `[3,4]`  

**Constraints:**
- `1 <= n <= 2 * 10^4`
- `edges.length == n - 1`
- `0 <= a_i, b_i < n`
- `a_i != b_i`
- All the pairs `(a_i, b_i)` are distinct.
- The given input is **guaranteed** to be a tree and there will be **no repeated** edges.

---

# todos

#/strats 
- check percentiles
	- https://leetcode.com/problems/minimum-height-trees/submissions/1270088162/
- dp?
	- https://leetcode.com/problems/minimum-height-trees/solutions/1631066/dp-on-trees-multiple-approaches-image/
	- https://leetcode.com/problems/minimum-height-trees/solutions/76052/two-o-n-solutions/
- jargon/abstraction: "closeness centrality"
	- https://leetcode.com/problems/minimum-height-trees/solutions/952374/python-3-approaches-detailed-explanation-and-visuals/




#/problems 
- [1203. Sort Items by Groups Respecting Dependencies](https://leetcode.com/problems/sort-items-by-groups-respecting-dependencies/)


#/move 
- lots of stuff in references


#/fundy 
- graph problems are like 3 steps:
	- step 1: modeling the problem
	- step 2: understand which strat to use (aka which standard graph algo/technique)
	- step 3: implementing the strat
		- no graph data structures in the standard library
		- so you need to code a lot of stuff by hand
	- arguably, what makes graph problems hard is steps 1 and 3
		- step 2 is arguably easier than other types of problems

#/fundy
- is this a good problem solving strat when you're totally confused?
	- imo the issue is: it seems so vague, 'first try a greedy approach'
	- maybe more specific: start with an element, and try to find the associated 'local best solution' via some kind of greedy/simple heuristic
	- and idea is: even if it doesn't work: it should help reveals some useful structure of the problem
- kinda...but imo, if i'm not able to figure out the 'structure' after just 1 attempt, then it means i'm not prepared enough
	- it's reasonable to just memorize patterns, given how simple these problems are
	- memorizing patterns/examples can take you absurdly far
		- to the point where, imo the only time i'd ever do "exploration + trying stuff and seeing why it doesn't work" is when you're literally doing research

#/problems 
- try to find a problem where you find the center of an arbitrary graph
	- wiki says you can use floyd-warshall
		- https://en.wikipedia.org/wiki/Graph_center


#/workflow 
- to copy/paste wikipedia text into obsidian:
	- if the text contains latex:
		- copy the text into the browser address bar; then copy the address bar text and paste into obsidian


#/problems
https://en.wikipedia.org/wiki/Degree_(graph_theory)



#/style
- order of precedence for 'optimal' solutions
	- time complexity
	- space complexity
	- 'coding' complexity
	- 'knowledge' complexity
- examples of 'knowledge' complexity being the deciding factor:
	- [[310. Minimum Height Trees]] 
		- both strats requires knowing the definition of a graph's center
		- the 'longest path' strat requires also knowing additional non-trivial facts:
			- tree only has 1-2 centers
			- these centers are the middle vertices on the longest path


#/problems 
- finding the path between two vertices of a tree
	- specifically i want problems that require you to return the path
	- and also: there is some kind of 'best' path
	- bc i'm interested in how exactly they write the traversal to carry/decide which info to keep/send to neighbors
- https://en.wikipedia.org/wiki/Distance_(graph_theory)
- https://en.wikipedia.org/wiki/List_of_graph_theory_topics
- [[Tutorial] Diameter of a tree and its applications - Codeforces](https://codeforces.com/blog/entry/101271)


# References


## #graphs 
- jargon
	- https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)#Graph
		- The order of a graph is its number |V| of vertices, usually denoted by n.
		- The size of a graph is its number |E| of edges, typically denoted by m.
			- in some contexts, such as for expressing the computational complexity of algorithms, the term size is used for the quantity |V| + |E|
		- The degree or valency of a vertex is the number of edges that are incident to it
		- two vertices x and y are adjacent if {x, y} is an edge.
		- Two edges of a graph are called _adjacent_ if they share a common vertex.
- https://en.wikipedia.org/wiki/Tree_(graph_theory)
	- jargon
		- An internal vertex (or inner vertex) is a vertex of degree at least 2. Similarly, an external vertex (or outer vertex, terminal vertex or leaf) is a vertex of degree 1. A branch vertex in a tree is a vertex of degree at least 3.
- https://en.wikipedia.org/wiki/Degree_(graph_theory)
	- A vertex with degree 1 is called a leaf vertex or end vertex or a pendant vertex, and the edge incident with that vertex is called a pendant edge.
- seems informal jargon:
	- neighbor/neighbours of a vertex, are the adjacent vertices


## #trees 

Note that for a tree we always have `V = n`, `E = n-1`.
(here, helps for complexity analysis)


## #graphs/center
- https://en.wikipedia.org/wiki/Graph_center
	- the set of all vertices of minimum eccentricity, that is, the set of all vertices u where the greatest distance d(u,v) to other vertices v is minimal. Equivalently, it is the set of vertices with eccentricity equal to the graph's radius. Thus vertices in the center (central points) minimize the maximal distance from other points in the graph.
- https://en.wikipedia.org/wiki/Tree_(graph_theory)#Properties
	- Every tree has a center consisting of one vertex or two adjacent vertices.
	- The center is the middle vertex or middle two vertices in every longest path.
- https://en.wikipedia.org/wiki/Centered_tree
	- a centered tree is a tree with only one center, and a bicentered tree is a tree with two centers
		- it seems the proof might not be trivial...so just memorize this for now
		- the proof is probly vaguely:
			- clearly, the best nodes to pick on a longest path are the center nodes
			- picking nodes outside of a longest path:
				- well then the distance from these nodes to endpts of a longest path:
					- suboptimal bc you have extra distance required to reach a longest path
- https://en.wikipedia.org/wiki/Distance_(graph_theory)
	- The diameter d of a graph is the maximum eccentricity of any vertex in the graph. That is, d is the greatest distance between any pair of vertices


- follow-up questions/observations
	- in a tree, the number of centers depends on parity of diameter; not on parity of the order


- why centers = roots of mht
	- bc height of a node's mht is exactly it's max distance to other nodes
- so then this problem is just about finding the centers





## #graphs/longest_path

- in general, this is a hard problem
	- https://en.wikipedia.org/wiki/Longest_path_problem
	- [[../_secondary/02 algorithms/skiena.pdf#page=386|skiena, 11.6 The Art of Proving Hardness]]
	- [[../_secondary/01 competitive programming/halim2.pdf#page=182|halim2, 8.6.5 Longest-Path]]
- specific situations with efficient algos
	- dag: critical path
		- [[../_secondary/02 algorithms/clrs.pdf#page=641&selection=44,66,115,10|clrs, page 641]]
		- [[../_secondary/02 algorithms/skiena.pdf#page=545&selection=36,0,59,9|skiena, page 545]]
		- [[../_secondary/01 competitive programming/halim1.pdf#page=279&selection=141,0,154,17|halim1, page 279]]
	- tree
		- [[../_secondary/01 competitive programming/halim2.pdf#page=182&selection=171,0,171,21|halim2, page 182]]
			- in a tree, 'longest path' = diameter
			- we know algos to find diameter

- even for undirected unweighted graphs, it's nontrivial
	- https://codeforces.com/blog/entry/4116


## #graphs/diameter

- diameter is NOT the same as longest path
	- diameter = longest shortest path
	- the thing about tree is:
		- each pair of vertices has unique path
		- so the path IS the shortest path


- finding diameter
	- general graph
		- [[../_secondary/01 competitive programming/halim1.pdf#page=276&selection=9,0,9,31|halim1, page 276]]
	- (weighted) tree
		- [[../_secondary/01 competitive programming/halim1.pdf#page=286&selection=146,0,146,27|halim1, page 286]]
			- 2x dfs or bfs
		- [[../_secondary/02 algorithms/clrs.pdf#page=585&selection=212,0,276,79|clrs, page 585]]
			- clrs puts this problem in the bfs section
			- solution
				- https://walkccc.me/CLRS/Chap22/22.2/#222-8-star
			- i suspect: bfs is the 'natural' solution to this problem bc:
				- bfs finds shortest distances
				- and use the fact that trees have unique paths
				- so you just 'add stuff' together...?
		- [[../_websites/codeforces/[Tutorial] Diameter of a tree and its applications.html|[Tutorial] Diameter of a tree and its applications]]
			- 2x dfs, with proof
		- 2x bfs, with proofs
			- https://cs.stackexchange.com/questions/22855/algorithm-to-find-diameter-of-a-tree-using-bfs-dfs-why-does-it-work
			- https://stackoverflow.com/questions/20010472/proof-of-correctness-algorithm-for-diameter-of-a-tree-in-graph-theory






- so for trees:
	- 2x dfs algo:
		- init dfs at any vertex `a`
		- return a vertex `b` with max distance
		- init dfs at `b`
		- return a vertex `c` with max distance
		- then `(b,c)` are endpts of a diameter
	- thus
		- our dfs routine:
			- needs to compute/return distance

- implementation examples
	- https://www.geeksforgeeks.org/diameter-tree-using-dfs/
		- this seems terrible lol




## #graphs/kahn 

- refs
	- [[../_secondary/01 competitive programming/halim1.pdf#page=231&selection=115,0,115,16|halim1, page 231]]


#/jargon 
- is this really 'bfs'?
	- seems more like 'peeling an onion', but i can't find any official jargon for this
		- literally the only mention i could find:
			- https://www.sciencedirect.com/science/article/abs/pii/S0377221720307001
		- so how come everyone in the lc forum solutions keeps saying this phrase...
	- #/research go check halim
	- for now, i've put this under 'kahn'
	- maybe because there's probly only 2 patterns/strats that could be described by 'peeling an onion' in a graph: 
		- if next 'peel' = all nodes adjacent to previous peel
			- then this is bfs
				- initialized on leaves
				- (bfs on root isnt really 'peeling onion', bc ur starting at the 'core')
				- find #/problems  for this
		- if next 'peel' = remaining leaves
			- then this is kahn
				- technically don't even need a queue for this
					- could easily just have a list of 'currPeel' and 'nextPeel', and just set them equal after going through currPeel and emtpy the nextPeel at start of new peel routine
				- but queue is nice bc in python (and i suspect, most other languages):
					- because of the way that 'for loops work on state' ( #/jargon)
					- you can just add the next 'peel' to the queue, as you iterate through the queue's state before this for loop started
					- this should be a #/fundy btw, for working with queues
		- if 'peeling onion' really is just either bfs or kahn:
			- then probly important to point out in kahn:
				- when/why use this on undirected graphs
		- also: can kahn be considered a variant of bfs?

- btw remember, one reason why kahn is so efficient:
	- at each peel, you never actually 'search through all nodes to find the leaves'
		- can just do this as you delete leaves and update edges:
			- check the neighbors degree after you delete the edge: and if its now a leaf, you add it to the queue

- another reason maybe this really is kahn:
	- can consider it like top sort, even tho undirected graph
	- bc it's implicitly a directed graph:
		- you assume a direction when you start pruning leaves, a direction inwards towards the center of gravity


---





- why each level is based on degree
	- and not based on: 'neighbor of last level leaves'


- i think the inspo is:
	- at each step, you delete what CANT be the center
		- aka delete leaves, bc we know that it's neighbor definitely has smaller distance to all other nodes 
		- so we're kind of 'flipping the problem'

- follow-up question is:
	- why this logic can be applied repeatedly
		- it's obvious it's true on the original graph

- another way to see why its based on degrees:
	- it's not necessarily true that: neighbor of leaves, are the next leaves...or 'larger distance'
	- observe that the node that og had indegree 4: is not deleted during second step, even tho its a neighbor of an og leaf node
	- ![[../!assets/attachments/Pasted image 20240529013922.png]]
	- ![[../!assets/attachments/Pasted image 20240529013931.png]]




## #graphs/adjacency_list 

- python: seems you typically use
	- `defaultdict(list)`
	- `defaultdict(set)`
	- is one better than the other?



## #python/modules/collections/deque 
- refs
	- https://docs.python.org/3/library/collections.html#collections.deque
	- https://realpython.com/python-deque/
- syntax
	- q = deque()
	- q.popleft()
	- q.append()
	- for _ in range(len(q)):
		- iterate over the current size of q
		- aka what is currently in the q, at start of the loop
		- aka current level




# Strategies


## longest path

- strat/inspo:
	- centers are the centers of a diameter
	- do 2x dfs to find a diameter's endpts



- see #graphs/diameter 
	- need to do 2x dfs
- but also:
	- need to modify dfs routine to also store path/parents
		- bc we need to know the nodes along the path btwn `b` and `c`
			- to return centers of that path
		- i suspect: saving a path is a very common thing, so it'd be good to know best practices here?
			- and maybe it's a fundy
		- ie is path or parent better?
			- the 'parent' approach feels more sus?
				- it's really clean in that you can just follow the parents recursively
			- but something about the 'paths' approach saving the shortest path...this seems like the preferred option...ie would actually be useful to more problems


- complexity
	- time O(n)
		- dfs twice
	- space O(n)



### using parents

```python
class Solution:
    def findMinHeightTrees(self, n, edges):
        def dfs_helper(start, n):
            self.dist, self.parent = [-1]*n, [-1]*n
            self.dist[start] = 0
            dfs(start)
            return self.dist.index(max(self.dist))
        
        def dfs(start):
            for neib in Graph[start]:
                if self.dist[neib] == -1:
                    self.dist[neib] = self.dist[start] + 1
                    self.parent[neib] = start
                    dfs(neib)
                    
        Graph = defaultdict(set)
        for a,b in edges:
            Graph[a].add(b)
            Graph[b].add(a)
        
        ind = dfs_helper(0,n)
        ind2 = dfs_helper(ind, n)
        
        path = []
        while ind2 != -1:
            path.append(ind2)           #backtracking to create path
            ind2 = self.parent[ind2]
            
        Q = len(path)
        return list(set([path[Q//2], path[(Q-1)//2]]))
```


### using paths
```python
class Solution:
    def findMinHeightTrees(self, n, E):
        G, seen = defaultdict(set), [False]*n
        for u,v in E:
            G[u].add(v)
            G[v].add(u)

        def dfs(i):
            if seen[i]: return []
            longest_path = []
            seen[i] = True
            for adj in G[i]:
                if not seen[adj]:
                    path = dfs(adj)
                    if len(path) > len(longest_path):
                        longest_path = path
            longest_path += [i]
            seen[i] = False
            return longest_path

        path = dfs(dfs(0)[0])
        return set([path[len(path)//2], path[(len(path)-1)//2]])
```


### do not do this (understand why, move to #/fundy )

complexity:
- The dfs function returns vector/list. Won't the time complexity be O(N2) due to copy made while returning each time?
	- For python, assignments and returns from a function are shallow copy of objects (unless you explicity specifiy for a deep copy). Thus we arent completely copying a path anytime in this case either.

aka importantly, do not do this, because higher time complexity:
Althought it works, it results in returning a new copy of list everytime resulting in higher runtime and space usage. The below solution will pass but its time complexity should probably be denoted as `O(N2)`
ie Since the DFS call is essentially returning a vector which would take linear time.

```python
class Solution:
    def findMinHeightTrees(self, n, E):
        G, seen = defaultdict(set), [False]*n
        for u,v in E:
            G[u].add(v)
            G[v].add(u)

        def dfs(i):
            if seen[i]: return []
            seen[i] = True
            longest_path = max((dfs(adj) for adj in G[i]), key=len, default=[]) + [i]
            seen[i] = False
            return longest_path

        path = dfs(dfs(0)[0])
        return set([path[len(path)//2], path[(len(path)-1)//2]])
```




## kahn


- high level:
	- remove leaf nodes, along with their edges, to expose a new layer of leaf nodes
		- track/update degrees to identify leaves
	- queue used for the 'trimming' process




- strat
	- build adjacency list
	- compute degrees
	- push all leaves (deg 1) to the queue
	- now we peel:
		- for each node in the queue:
			- remove node
			- update degree of its neighbor
			- add neighbor to queue if it's now a leaf
	- during above loop, keep track of how many vertices are left
		- if none, then the queue contains the centers
		- so actually: don't even need to know/use that there's only 1-2 centers

- complexity
	- time
		- O(V+E) = O(n + (n-1)) = O(n)


- time O(V+E)
	- O(V+E) to construct the graph representation
	- ? O(V) for the degree array
	- O(V) to initialize the queue
		- (it's possible that all nodes, except center, are leaves)
	- while loop
		- processes each node once when it becomes a leaf, decrementing the degrees and potentially adding new leaves to the queue. Every edge is looked at exactly twice (once for each vertex it connects) over the entire runtime of the algorithm. So the complexity associated with this part is `O(E)`.

- space O(V+E):
	- graph representation `g`
		- O(E): list of neighbors for each node
			- aka 2x num edges
	- O(V) to store the degree of each vertex
	- O(V) for the queue

---

- special/base/corner ( #/jargon ) case:
	- if `n==1`

- graph rep
	- use adjacency list `g = defaultdict(list)`
		- `g[v] = list of v's neighbors`
- list `degree` to track degrees of each node

- For each edge `(a, b)` in the `edges` list
	- add `b` to the adjacency list of `a` (i.e., `g[a].append(b)`) and vice versa because the graph is undirected
	- Increment the degree of both `a` and `b` since the edge contributes to the degree of each

- init a deque with all leaves






### example 1
```python
class Solution:
    def findMinHeightTrees(self, n, E):
        if not E: return [0]
        G, seen = defaultdict(set), [False]*n
        for u,v in E:
            G[u].add(v)
            G[v].add(u)
        leaves, new_leaves, in_degree = [], [], []
        for i in range(n):
            if len(G[i]) == 1:
                leaves.append(i)
            in_degree.append(len(G[i]))
        while n > 2:
            for leaf in leaves:
                for adj in G[leaf]:
                    in_degree[adj] -= 1
                    if in_degree[adj] == 1:
                        new_leaves.append(adj)
            n -= len(leaves)
            leaves = new_leaves[:]
            new_leaves.clear()
        return leaves
```


### example 2

???
- While `q` is not empty meaning there are still leaves to remove, clear the list `min_height_trees` to prepare for a new set of leaves.
	- why is this even used


```python
# https://algo.monster/liteproblems/310
class Solution:
    def findMinHeightTrees(self, n: int, edges: List[List[int]]) -> List[int]:
        if n==1:
            return [0]
        
        graph = defaultdict(list)
        degree = [0] * n

        for node1, node2 in edges:
            graph[node1].append(node2)
            graph[node2].append(node1)
            degree[node1] += 1
            degree[node2] += 1

        leaves_queue = deque(i for i in range(n) if degree[i] == 1)
        min_height_trees = []

        while leaves_queue:
            min_height_trees.clear()
            for _ in range(len(leaves_queue)):
                current_node = leaves_queue.popleft()
                min_height_trees.append(current_node)
                for neighbor in graph[current_node]:
                    degree[neighbor] -= 1
                    if degree[neighbor] == 1:
                        leaves_queue.append(neighbor)

        return min_height_trees
```


### delete if only difference is 'lists instead of queue'

```python
def findMinHeightTrees(self, n, edges):
    if n == 1: return [0] 
    adj = [set() for _ in xrange(n)]
    for i, j in edges:
        adj[i].add(j)
        adj[j].add(i)

    leaves = [i for i in xrange(n) if len(adj[i]) == 1]

    while n > 2:
        n -= len(leaves)
        newLeaves = []
        for i in leaves:
            j = adj[i].pop()
            adj[j].remove(i)
            if len(adj[j]) == 1: newLeaves.append(j)
        leaves = newLeaves
    return leaves
	
```



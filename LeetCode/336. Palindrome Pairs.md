[336. Palindrome Pairs](https://leetcode.com/problems/palindrome-pairs/)

```python
class Solution:
    def palindromePairs(self, words: List[str]) -> List[List[int]]:
        
```

# Description

You are given a **0-indexed** array of **unique** strings `words`.

A **palindrome pair** is a pair of integers `(i, j)` such that:
- `0 <= i, j < words.length`,
- `i != j`, and
- `words[i] + words[j]` (the concatenation of the two strings) is a palindrome. A **palindrome** is a string that reads the same forward and backward.

Return _an array of all the **palindrome pairs** of_ `words`.

You must write an algorithm with `O(sum of words[i].length)` runtime complexity.

**Example 1:**  
**Input:** `words = ["abcd","dcba","lls","s","sssll"]`  
**Output:** `[[0,1],[1,0],[3,2],[2,4]]`  
**Explanation:** The palindromes are `["abcddcba","dcbaabcd","slls","llssssll"]`  

**Example 2:**  
**Input:** `words = ["bat","tab","cat"]`  
**Output:** `[[0,1],[1,0]]`  
**Explanation:** The palindromes are `["battab","tabbat"]`  

**Example 3:**  
**Input:** `words = ["a",""]`  
**Output:** `[[0,1],[1,0]]`  
**Explanation:** The palindromes are `["a","a"]`  

**Constraints:**
- `1 <= words.length <= 5000`
- `0 <= words[i].length <= 300`
- `words[i]` consists of lowercase English letters.

---


# todo


### summary
- the "(a+b) only" strat is definitely the most intuitive at a high level
	- however the issue is that it's implementation is most complicated:
		- it requires the most complicated structure
		- not just trie: but trie that stores both palindrome and full word info
	- well tbh..not THAT complicated...you figure out what you need, and then just store it
- imo the other strats (ie choosing one out of a+b and b+a) is more complicated to explain on a high level, so imo these should come later


## cleanup
- put far more headings below, so that it’s easier to edit/move stuff while it’s currently in this incredibly rough state




## brute force
- brute force is: for each pair of strings, concatenate and check if it’s a palindrome
- i wonder if each major strat is like a different interpretation of how to optimize from brute force
	- hashmap
		- tbh i’m not sure what this is optimizing
			- it’s true that there are problmes where: ‘check if valid inverse exists among stuff we’ve iterated over’ is more efficient than ‘check pairs and see if its valid’
			- but the big issue is that: each element requires a bunch of ‘inverse’ checks, not just one
			- and isn’t ‘reversing’ a string is theoretically same complexity as checking for a palindrome
				- bc both involve linear scans?
	- trie
		- seems like this approach actually does optimize…bc it optimizes both:
			- ‘check if palindrome’
			- ‘check if exists over stuff we’ve iterated over’


## first attempt at understanding trie and hash
- hashing
	- for each word:
		- for each prefix and suffix:
			- check if it matches with any word we’ve already visited

- trie
	- one strat seems to be:
		- first build the entire suffix tree
		- for each word:
			- for each prefix:
				- check if it matches with any suffix
	- this suggests: maybe there is room for a time optimization, by only checking over stuff we’ve visited?
		- however: then this increases space, bc now, we probly need a tree for both prefix and  suffix


### trie allows for pruning
- !! so it seems there’s like multiple optimizations going on here:
	- first is the fact that trie is better than hashmap, bc it prunes a search far earlier
		- ie no point checking larger prefixes, if we already know it can’t be built


### can we build as we iterate?
- second possible optimization is:
	- if we build our structure as we go along:
		- aka only check among visited
		- then we need to run checks on both prefixes and suffixes
	- but if we build structure ahead of time:
		- we could just only run checks on prefixes
	- !!!!!wait actually no....
		- ie https://leetcode.com/problems/palindrome-pairs/solutions/2585453/python-hashmap-simple-and-intuitive/
			- even if you build ahead of time, depending on your algo, you might still need to do prefix AND suffix checks
			- this guy needs it bc: the 'mid' part can only be in current word
			- and this can happen to ways...either current word is the 'first part' (so then mid is the suffix)
				- or current word is the 'second part' (so then mid is the prefix)
		- point is: if your strat is:
			- hash the reverse of every word
			- iterate over words
				- for current word: this is the word that can 'split'
				- and 2 ways: either prefix + mid
					- or mid+suffix
				- and for both ways, you need:
					- mid is a palindrome
					- reversed prefix/suffix exists as a word in our 'dictionary'
						- btw 'dictionary' means like the input list of words; is not the python dictionary type
						- probly more efficient:
							- have the hashmap store reverse of the words
							- ie so that we can just immediately check slices, instead of having to reverse every slice and check it...
							- so this is like a 'preprocessing' type efficiency...(what's the #/jargon)


			- ??? wait so what was i even thinking of earlier....
				- cuz based on above...it doesn't actually seem like you can 'build hashmap as you go'
					- aka this is probly important variant/counterex for understanding when to #hashing/collisions 
						- ie the 'existence' checks need to be symmetric...ie 'a colliding with b' and 'b colliding with a' need to have equivalent outcome
						- #/meta counterexamples for when you CAN'T use a technique will surely be very important as well, so make sure to point these out
							- ie for #hashing/collisions , see [[336. Palindrome Pairs]] for why we need 'symmetric collisions'
						- btw #hashing/collisions i should definitely point this out
							- i don't think these 2 cases (symmetric, non-symmetric) need to be variants
							- just briefly say: if symmetric, then you can build as you go
							- if non-symmetric: then just buid it at the beginning
							- tho tbh..maybe just always build it at the beginning? bc iirc, time complexity there's no difference...and i'm not even sure the 'constant difference' even exists, bc hashing is presumably O(1), so it doesn't matter that ur trying to find something in a larger hashmap
							- and 'building as you go': the presumed benefit is that maybe, you wont need to do all iterations...so you wont need to 'waste time' building whole hashmap
							- but if you 'build everything at the start': then higher chance you find a collision earlier...
							- so yea, maybe just say: default to building hashmap at very beginning
							- and i can briefly mention: if symmetric collisions, then you can also build as you go...but it's debatable whether this is even an optimization
								- it's only an optimization in terms of 'reduced number of passes'
				- if we build as we go, ie so iteration a comes before iteration b:
					- then it means:
						- b wont exist in hashmap when we iterate over a
						- a exists in hashmap when we iterate over b
				- but this is a probly, bc what we're doing isn't a 'symmetric' check:
					- ie if we have solutions involving a and b, then this basically happens in 2 distinct ways:
						- a splits into pre/suffix and mid ,and gets paired with b
						- b splits into pre/suffix and mid, and gets paired with a
						- (distinct, aka these are not equivalent, aka not symmetric)
					- and point is, if you build as you go:
						- you miss the first option
						- you only get the second option

	- (so this next point is probly wrong for this problem, but probly still a fundy for a diff concept)
	- !!! so this is an example where, if the ‘pairing/inversion’: if order matters:
		- (order matters here, bc if input is `[a, a]`, then both 01 and 10 are considered distinct palindrome constructions)
		- then ‘only checking over visited’ will increase how much code you write, bc now you have to write a subroutine for both options/orders
		- i feel like complexity of both is the same though…’only checking over visited’ will probably have a better constant? but now you have to write like double the code
			- wait actually…i don’t think it has a better constant
			- bc ‘only checking over visited’ still ends up creating the full structure

### pruning optimization

- the big reason why trie is more attractive than hashmap:
	- when we check existence of a prefix/suffix:
		- if it dne in trie, then we know there’s no point checking larger prefix/suffix, bc they don’t exist either
		- but hashmap does not allow for this pruning step
- !! so the optimizations really have nothing to do with: ‘checking if it’s a palindrome’
	- well they do, but this basically already built into the fact that you gradually increase the prefix
		- increasing a prefix/suffix is O(1)
		- and then you just check existence for the match, this is O(1) for both trie and hashmap
	- however this ‘optimization’ comes at a heavy cost:
		- bc in brute force, you never had to consider prefix/suffix
		- but now for these ‘optimized’ solutions, you do have to consider every prefix/suffix
		- actually these ‘optimized’ solutions are arguably worse, bc you also need to do a palindrome check on the rest of the word, ie the ‘mid’ part
- ok so honestly…idk if the hashmap approach is even a good approach tbh…
	- aka this should really be considered a pure ‘trie’ problem
- surely there’s some kind of optimization for these ‘optimized’ approaches, for the part where you check if ‘mid’ is a palindrome?
	- ehh i doubt it…the problem is that the substring/mid palindrome has to start at an endpoint (ie its a prefix/suffix), and so i dont think there’s an efficient way to do this
		- ie can’t use manacher’s, aka it’s not like [[647. Palindromic Substrings]]
			- there, we had optimizations bc the palindromes could be anywhere in the string
				- aka that’s why, we do it by iterating over possible ‘centers’
	- and imo storing these computations (ie dp)…there’s no benefit
		- bc the only time you do a mid check on a substring, is when its full string is the current iteration
			- aka for each prefix/suffix of a string, you only do a ‘check if mid is palindrome’ only once
- another reason against ‘pre-computing’ whether each prefix/suffix is a palindrome
	- exactly bc it’s so expensive
	- aka, only do it if you need to
		- aka if you’ve already verified that the ‘reversed’ of the other part of the string, already exists in the input

- !!wait..i need to confirm that pruning is actually possible when you use a tree..
	- above, i basically handwaved why it works..but tbh i think i should def work through an example to confirm
	- suppose current word iteration is on word `abcdc`
		- and current prefix iteration is `ab`
		- so we need 2 things:
			- check if `cdc` is a palindrome
			- check if `ba` exists in `words`
				- that is: `ba` is in the trie, AND it is a word
	- crap…i think pruning does exist, but nor in the way i think it does
		- if we use a trie, we can def prune wrt..checking if `ba` exists in words
			- so pruning would happen here if `b` doesn’t exist in the trie
			- but this isn’t that useful tbh…hashing does this check in O(1)
		- versus pruning in the style of the handwaving i was doing above, was saying that:
			- if `ba` doesn’t exist in the trie, then we can move onto the next ’word’ iteration
				- aka skip further ‘prefix’ iterations
			- but is this true?
			- the next prefix iteration would be on `abc`
			- and so we need
				- check if `dc` is a palindrome
				- check if `cba` exists in `words`
		- ohhh i think pruning does exist in the way i was thinking! but need to be careful
			- point is that we need to design the trie so that:
				- if we know `ba` doesn’t exist in the trie, then we know `cba` doesn’t exist in words
					- ie that `cba` isn’t in the trie either
			- ohhh this is subtle:
				- the trie just stores the words normally
				- but downward edges corresp to the char being added to the FRONT of the string
					- ie parent is the next/‘largest nontrivial’ suffix of the child
				- so it’s not that you’re storing reversed words
				- it’s just, the downward edges are the ‘opposite’ of what we saw in other examples that use trie
				- oh is the jargon here: prefix vs suffix tree?
				- i suspect this is completely standard
				- nvm, it’s not!!
				- if you look at halim2:
					- what we’re doing really is the ‘reverse’ of the suffix trie
					- (which is NOT a prefix trie)
				- oh actually, it can be considered a prefix trie:
					- it actually is equivalent to a prefix trie of the reversed words
					- bc in above example, if we reverse the word checks: we're saying:
						- if `ab` doesn't exist in trie, then we know `abc` doesn't either
				- ok so based on what halim2 is implying:
					- suffix trie is same as prefix trie, in that:
						- you always append chars as you traverse downward
					- and suffix trie stores more bc:
						- prefix trie typically only indicates if a substring is a word (in the 'dictionary')
						- but suffix trie also indicates if a substring is a suffix (of a word in the dictionary)
							- this is because: not all nodes in suffix trie will corresp to suffixes!!!
								- !!!!



## references for tries

- (eventually move these all to the #trie tag)

- the following are all mentions of 'trie' and related structures, among my refs
	- why do the big texts lack much, if any, discussion?
	- is it bc they're literally only useful for some string problems?
		- that's kinda what halim implies
- cmu
	- [[../_courses/cmu/15-122-s15/22-tries.pdf|22-tries]]
	- [[../_courses/cmu/15-451-f20/lec24-sufftree.pdf|lec24-sufftree]]
- https://brilliant.org/wiki/tries/
	- tries = prefix/radix tree
- wikipedia
	- https://en.wikipedia.org/wiki/Trie
		- trie = prefix tree
	- https://en.wikipedia.org/wiki/Suffix_tree
		- suffix tree = trie containing suffixes
	- https://en.wikipedia.org/wiki/Suffix_array
		- space efficient alternative to suffix trees
	- https://en.wikipedia.org/wiki/Radix_tree
		- radix trie, compact prefix tree, compressed trie
		- a space-optimized trie (prefix tree) in which each node that is the only child is merged with its parent
- [[../_secondary/02 algorithms/clrs.pdf#page=349&selection=294,0,510,1|clrs, page 349]]
	- trie = radix tree
- [[../_secondary/01 competitive programming/halim2.pdf#page=74|halim2, 6.5 Suffix Trie/Tree/Array]]
	- suffix trie/tree/array are all different structures used to store 'all suffixes in a dictionary'
- [[../_secondary/02 algorithms/skiena.pdf#page=460|skiena, 15.3 Suffix Trees and Arrays]]
	- In its simplest instantiation, a suffix tree is simply a trie
	- a trie is a tree structure, where each edge represents one character, and the root represents the null string. Each path from the root represents a string, described by the characters labeling the edges traversed.
	- suffix arrays, which do most of what suffix trees do, but are easier to implement.
- [[../_secondary/02 algorithms/sedgewick.pdf#page=745&annotation=4526R|sedgewick, page 745]]
	- trie = type of search tree
- [[../_secondary/02 algorithms/kt.pdf|kt]] has no discussion
- [[../_secondary/02 algorithms/dpv.pdf|dpv]] has no discussion


## understanding prefix and suffix trees

- seems the diff btwn 'prefix trie' and 'suffix trie' is:
	- first, i actually think a  'trie' IS a 'prefix trie'
	- and so in any (prefix) trie: you are 'storing' a desired collection of strings
		- and point is: in the most basic case: this collection is exactly your list of words
		- but in a suffix trie: this collection is all suffixes of words
	- and also seems: traversing in any 'trie' is always considered as done via 'prefixes'
- observe then that 'suffix trie' on a list of words, is very massive compared to the basic 'trie'
	- given how big it is, it might seem odd that such a structure would ever be desired
	- but remember general principle: if you store it, then it means you can 'look it up' instead of computing it
		- and point of all these diff data structures, is that based on the application, you can design the structure so that 'finding what you need' is super fast
	- and it seems there are many important applications (ie string matching) where suffix trie stores things very optimally for you


- so basically, seems 'prefix trie' and 'trie' are the same thing
	- bc you should consider the edges/paths/nodes as always denoting 'prefixes'
	- ie you always increment in the same direction
	- versus: 'suffix tree/trie' is just pointing out that the 'things you use to build the trie' are suffixes
		- but you still store it via PREFIXES of the suffixes
		- so actually, a technically correct equivalent name is probly 'prefix trie of suffixes'
		- however, probly just say 'suffix trie' bc its such a common/important type of prefix trie
	- btw one reason for this discussion, is to basically point out that 'prefix trie' and 'suffix trie' are not 'complementary' whatsoever
		- ie, it's NOT the correct intuition that 'oh they're essentially the same thing, it's just one stores prefixes and the other stores suffixes)
		- bc key point is that you want to store suffixes via they're prefixes..which (probly) significantly increases space complexity
- btw...ppl seem to use prefix/suffix tree? instead of prefix/suffix trie? which one is it??



- if you want to search for a pattern among a list/'dictionary' of words, then a prefix tree won't work. bc what if the pattern is 'inside' a word. ie a prefix tree would only provide matches when the pattern is the start of the word.
	- this is why you need to store all suffixes: aka suffix tree


- quick example to see value of suffix tree:
	- allows you to check whether `s` is a substring of `t`
	- bc any substring of `t` can be equivalently considered: a prefix of a suffix of `t`
	- ie in `catdogfish`:
		- pattern `dog` is a prefix of the suffix `dogfish`


- A suffix tree can be viewed as a data structure built on top of a trie where, instead of just adding the string itself into the trie, you would also add every possible suffix of that string. 
- This is the simplest and slowest way to build a suffix tree. It turns out that there are many fancier variants on this data structure that improve on either or both space and build time.
- suffixes of the same string, they share a lot of information, so there are optimized algorithms that allows you to create them more efficiently. Ukkonen's algorithm, for example, allows you to create a suffix tree online in O(n) time complexity.



#/fundy 
- i should probly find a problem that requires suffix trie
	- but not urgent, and likely don't need more than 1



## almost certainly not using a suffix tree
- based on the above:
	- !!! the correct jargon is NOT 'reversed suffix trie'
		- it is: 'prefix trie of the reversed words'



## brute force might be optimal for some test cases
interesting comment by czheng in the discussion section of this problem:
https://leetcode.com/problems/palindrome-pairs/description/comments/1566204


The time complexity of the official solutions will be much worse than simple brute force in some edge test cases, most likely will trigger TLE. Consider the following test case:
```
["aaaaa...(repeat 149 times)**bc**aaaaa(repeat 149 times again)", "aaaaa...(repeat 149 times)**bb**aaaaa(repeat 149 times again)",""]
```

Note this test case has only 3 elements, where the first one is not a palindrome, the second one is a very long palindrome, and the third one is empty (thus is also a palindrome, and will combine with the second one to form a valid output). The answer should be `[[1,2],[2,1]]`.

In brute force the time complexity is O(n^2 x k) = O(3^2 x 300) = O(2700). But in both hashmap and trie solution the time complexity is O(n x k^2) = O(3 x 300^2) = O(270000), which is 100 times of the brute force solution.

Now this single case may be ok for the timing of the online judge. But in an extreme scenario if there are 1000 such test cases input to the program, the total runtime will be significantly increased, most likely will trigger TLE.

Another interesting thing is it seems C++ is the primary victim of such timing judge. Sometimes if we just write a quick but dirty code without recycling the resources (e.g. the new TrieNode created during the trie building), the timing will be ok. But if we explicitly record all allocated resource and manually free them at the end of the program, the timing will exceed the limit of the online judge. This sounds very unreasonable -- the timing is too tight which discourages programmers to properly recycle the allocated resources. This maybe fine for Java but will be a disaster for C++ in realworld.


## strat: kmp
- actually, there might be an even better solution
	- https://stackoverflow.com/questions/66556932/stuck-on-how-to-make-this-palindrome-pairs-finding-function-be-less-than-on2
		- uses kmp
		- somehow is able to find all palindromic prefixes in linear time
		- if this is true, maybe then they use this so that: they build a suffix tree only out of suffixes where the prefix is a palindrome
			- (idk if this is actually what they’re saying, i’m just rambling; if it’s not, then maybe still look into whether this strat has any legs)
			- so then your strat is like the ‘opposite’ of the ‘reversed prefix trie’ approach
				- ie: after building the trie:
					- instead of: current word iteration is the word we split
						- remember, i thought that you basically have to iterate like this, bc i didnt know there is efficient way to compute all palindrome prefix/suffixes
					- rather: current word iteration is the word that is NOT split
						- (bc we stored stuff in suffix tree so that it would be split)
				- obvi the space complexity will be ridiculous
				- is this better time complexity though? bc basically kinda skip the palindrome check now

## jplank on tries and hash
- https://web.eecs.utk.edu/~jplank/topcoder-writeups/Leetcode/Palindrome-Pairs/index.html

### strat 1

- prefix trie of the words
	- mark node with indices of words where:
		- the suffix (aka, part of the word without the prefix indicated by this node) is a palindrome
- then the strat is:
	- iterate over words
		- let the current word be `a`
		- we look for solutions `(b,a)` where:
			- b = ra+pal
			- ie so the solution is b+ar = ra + pal + a
		- so this involves: traversing this trie by processing the word backwards
			- ie we traverse the trie to find node `ra`
			- then check if `ra` has any associated indices..bc then this denotes which words `b` that are `ra+pal`
- however this strat misses words of the form:
	- (!!!! now i’m thinking…i think i was wrong in some of my earlier ramblings..you do need to construct both the forward and reverse tries at some point)
	- b = pal + ra
	- aka solutions a + b = a + pal + ra
	- aka the above strat only does the case where:
		- current word is not split, and it is the suffix of the concat
	- we still need the case:
		- current word is not split, and it is the PREFIX of the concat
	- (note: we don’t need the case where ‘current word is split’..bc that will be addressed when iterating over other words)
	- this means: you need to run this strat again, and symmetry/mirror stuff:
		- prefix trie of the reversed words
		- for each word: traverse tree in normal order


### strat 2
- strat:
	- removes the palindrome info in the trie
	- as he mentions:
		- saves storage bc no palindrome info
		- moves palindrome checking into the actual routine, which should be faster bc less naive palindrome checks
			- it also means, the current word is the one being split
			- ie if current word is `a`, then we use trie to find `b` such that:
				- `a + b = rb + pal + b`
				- aka `a = rb + pal`
			- (and symmetry for finding b+a)
	- (this is the approach i basically had in mind, when i read other ‘standard’ trie approaches)


### strat 3 (b+a only)
- strat3:
	- prefix tree that combines both ‘is word in dictionary’ info and ‘exist word where remaining suffix is palindrome’
	- when you combine the palindrome and end word info, the reason why you only have to do 1 pass:
		- you traverse tree via `ra`
		- for current word `a`, you’re finding `b` such that either:
			- along the path to (including) `ra`, if we have ‘end word info’:
				- then we found b such that:
					- ra = b + stuff
						- equivalently: a = rstuff + rb
						- aka: rstuff is just a prefix of `a`
						- so now we just check if stuff/rstuff is palindrome
					- if it is a palindrome, then we have solution:
						- b + a = b + pal + rb
						- because a = pal+rb
			- also at node `ra`, if we have ‘palindrome info’:
				- then we found b = ra + pal
				- so solution b+a = ra +pal +a
	- ahhh so this strat basically:
		- current word a is always the suffix
		- this trie structure lets us split both a and b
	- and so that’s why, we can do this with just 1 structure and 1 pass through words



### i should point how solutions can happen

- (the discussion below is possibly a very complicated example of #handling_duplicates )
	- #/jargon figure out the right tag for this btw
	- basically: i’m realizing now, ‘correctness’ of solution is actually kinda subtle..in that, how do you know you’ve found all solutions?
	- so what we did here:
		- understanding how solutions can happen
		- figuring out what solutions you actually need (ie what the problem considers to be distinct solutions)
		- then pick the cases so that you only find solutions once
			- so you dont need to hash the solutions as you build them to account for duplicates


- strats 1,2,3 (in particular 3) make me realize i should clarify:
- given a and b, the ways a solution can happen are:
	1. a = rb + pal
		- so a+b = rb + pal + b
	2. a = pal + rb
		- so b+a = b + pal + rb
	3. b = ra + pal
		- so b + a = ra + pal + a
	4. b = pal + ra
		- so a + b = a + pal + ra
- and the key observation is that:
	- there are obvi symmetries above, such as:
		- 1,2 are splitting a; 3,4 are splitting b
	- order does matter (ie a+b and b+a are DISTINCT solutions), BUT there are still redundancies in the above, ie likely only have to pick 2 things you want to search for, to find all solutions 
	- the way you realize what is redundant: you look at what the solutions look like!!
		- ie: 1,4 give you solutions a+b; and 2,3 give you solutions b+a
	- so what you need to search for is: pick one among 1,4; and pick one among 2,3
	- then based on what you want to find: you design your structure around this

- other key observations:
	- here, `a` is the ‘current word iteration’ and `b` is the word pulled from the structure
	- the word you split will require palindrome checks on prefix/suffixes
		- and based on which word you’re splitting, it will force what your structure needs to store/memoize
		- if you want to split `a`: 
			- then you need to know that `b` is a full word
		- so if you want to split `b`: 
			- note that palindrome checks on prefix/suffixes of `b` is something that can happen multiple times
			- so if you want to split `b`, then you should either precompute or memoize the palindrome checks
	- reversals
		- the word `a` is in its normal order, and so you’re looking for words `b` that overlap with ‘reversals’ of `a`
		- this means that either:
			- nSrT: structure holds normal words, and you need to find/traverse the ‘reverse’ of `a` in your structure
				- nSrT = normal structure, reversed traversal
				- if trie, then we can only traverse prefixes of b and full words b
				- so we cannot use this for a+b where b is split
					- aka cannot use it for aB
				- we CAN use it for
					- Ab, if trie stores full word info for b
					- bA, if trie stores full word info for b
					- Ba, if trie stores pal suffix info for b
			- rSnt: structure holds reversed words, and you find/traverse `a` normally in your structure
				- if trie: then we can only traverse suffixes of b and full words b
				- so we cannot use it for Ba
		- also now this makes it kinda clear why:
			- bc hash only stores full words:
				- at some point we will need to construct both structures and do both traversals
				- and also, hashmap can only do Ab and bA
			- (actually, i’m thinking of this backwards tbh..well not really, the point is that the hashing approach kind of forces that you can only do Ab and bA…because those are the only options that work)
		- oh so actually: yea, i should think of this as: again from before:
			- options tell you what your structure needs to store
			- normal/reversed structures have limitations on what they can store
			- so certain normal/reversed structures cannot be used with certain options
		- ie the choice of the options 1-4 does put limitations on the type of structure/traversal that you can do
			- aB: ra needs to be the suffix of b
				- so we need suffix info for b
			- Ab: rb is the prefix of a
				- we need full word info for b
			- etc etc for Ba and aB
- and so now, i can just classify strats based on what they’re picking to search for, and what structure they use to store `b`
	- and depending on the strat, if they precompute or memoize palindrome checks on `b`
	- for example: jplank strat 3 is:
		- look for options 2 and 3
		- use a trie
			- option 2: splits a, so we need to know if `b` is a word
			- option 3: splits b, so we need to know if pre/suffix of `b` is a palindrome
				- here, we precompute this info
		- so now it also becomes obvious why he can do it in ‘one iteration’ through words:
			- his structure is storing 2 things
- btw, imo i should rename these options bc i think i can come up with abbrevs that easily indicate what they’re doing:
	- aB = a + b, where we split b (so i write B because it’s like b is bigger, bc we split it)
	- Ab = a + b, split a
	- Ba = b + a, split b
	- bA = b + a, split a
	- and so then it’s very easy to confirm a strat is correct…ie easier to see that aB + Ba will work, instead of being like ‘option num1 and option num2’
- btw, how to cleanly explain how the AB and BA situations are taken care of? ie full word for both?
	- ehh imo this is clear…there’s obvi no duplicates because the order is different
		- and they’re obvi found as long as you let trivial splits be valid
		- ie we let the whole word be considered a prefix/suffix

### strat 4
- hashing, and always split `a`
	- so this is Ab and bA
	- Ab:
		- construct a new hashmap of normal words
		- then iterate through words
			- traverse a in reverse
	- bA:
		- same thing, but mirrored
- it’s interesting that this is faster, even tho he’s using c++
	- so it might not be a ‘python loop’ thing that’s causing the trie solutions to be slower in practice



## codechef
- https://discuss.codechef.com/t/pp-editorial/11769
	- note: this problem only cares about number of solutions, not the actual ordered pairs
		- so the structure can associate counts with keys (words/substrings), instead of the indices of the word/substrings
		- also seems this problem allows duplicate words
	- trie
		- normal structure
		- `uptill` = full word info
			- number of words that are this word
		- `below` = palindrome info
			- current node is prefix
			- number of words where remaining suffix is a palindrome
	- strat:
		- step1: getanswer(w)
			- (btw: at this point, `w` is now what i’ve been calling `a`, during my jplank analysis)
			- remember: trie is normal words, so we do reversed traversal on `w`
			- at each node along traversal, we use `uptill` to compute how many solutions of the form bA
				- note: this means, need to do palindrome check on prefixes of a
			- also at final node of traversal:
				- we use `below` to compute number of solutions Ba
		- step2: insert(w)
			- normal structure, so we insert normal word
			- add palindrome and ‘full word’ info for the word
				- at full word: we add to `uptill`
				- palindrome info: at prefix: we update/add to `below` if suffix is a palindrome
					- this means we need to do palindrome checks on suffixes of a
					- so for every word, we do 2 passes of palindrome checks…once during getanswer (on prefixes); and once here for insert(w), on suffixes
	- !!! they build the trie as the iterate through
		- oh it’s because, he does the above for all options
			- can check the editorialist’s code, ie the guy who wrote the solution
		- his explanation is basically just: assuming b+a, then how do we find these
			- aka he assumes you realized, you do this process also for a+b case
		- so he’s basically going through all 4 options
			- so actually, this means my earlier discussion of #handling_duplicates wasn’t actually correct
			- bc this guy is able to ‘only check collisions among what we’ve iterated’ over, and it works
			- because he’s basically checking redundant collisions, by going through all 4 options
			- but it ends up not being redundant, bc you avoid redundancy by only storing things you’ve iterated over
			- aka there’s like 2 sources of redundancy, and you just have to make sure you avoid one of them. ie pick technique:
				- only check collisions over what you’ve iterated over…ie streaming
				- redundant solutions

- #/cleanup lol…how am i ever going to make this writeup clean…all these strats seem interesting/useful to think about

### g4g: rolling hash
- aka efficient palindrome prefix checking
- https://www.geeksforgeeks.org/online-algorithm-for-checking-palindrome-in-a-stream/
- it seems you have to do this manually
	- so i’m worried about ‘inefficient in practice bc python loops’
- but theoretically, this seems definitely better:
	- the point is you keep a running hash value for the ‘first half so far’ and ‘second half so far’
		- it’s just a formula, so in python, just have ‘counts’ structure
	- when you add a char: then you really only need to update like 2 chars worth of computations for first half and second half
		- bc first half:
			- maybe add a char at end
		- second half:
			- add a char at end
			- maybe remove char at front
	- if hash vals/structure match, then you actually do a palindrome check
	- aka the point of the hash here, is to just keep track of counts of chars
		- makes sense: hash function has no way of knowing where the chars actually occur
		- but expect lots of efficiency, bc other than rly wacky strings: there shouldnt be that many places where the running prefix has ‘same char counts’ in each half of the prefix


## !!! duplicates is subtle!

- (remember: a is the word for ‘current iteration’, and it is trying to find solutions b)
- example 1: suppose `lls` and `s` are in our dictionary
	- when we run routine to find b+a:
		- then during iteration a=`lls`, it will find solution with `b=s`
		- ie add the solution (s, lls)
	- but when we run routine to find a+b:
		- then during iteration `a=s`, it will find solution with `b=lls`
		- aka it will also add the solution (s, lls)
	- !! but actually, this is exactly because neither `lls` and `s` are being ‘split’
		- see next example
- example 2: suppose `lls` and `sssll` are in `words`
	- this will generate an ordered solution `(lls, sssll)`
		- because `lls + sssll = lls + ss + sll`
		- in particular, `sssll` is the nontrivial split
	- aB: `lls` will find `sssll` 
	- Ab: `lls` will NOT find `sssll`
		- because we only allow for splitting a
	- bA: `sssll` will find `lls`
	- Ba: `sssll` will NOT find `lls`
		- because we only allow for splitting b
- !!! ok so actually,  to handle duplicates, i think you either:
	- do some kind of tactic where:
		- only check collisions over stuff you’ve iterated over
		- (this needs to be thought about more)
	- or: need to make sure that:
		- (based on example 2):
			- pick aB and Ba, or pick aB and Ba
				- aka: not just pick one out of a+b and b+a
				- but also, fix which of a and b are being split
		- (based on example 1)
			- AND: after picking your a+b and b+a routines:
				- one of the routines needs to ignore all solutions with trivial splits (aka no splits)
		- see some of the current hashing example codes below, this is why the subroutines for a+b and b+a have slight differences
			- namely, the second subroutine will have an added condition to ignore the trivial splits
	- !!! actually there's another way to handle duplicates, see jplank3 above and 'a+b only' below
		- pick both a+b, or both b+a
			- so i think: you need to either fix what is being split, or fix the order

## a+b only; trie
- code example
	- https://leetcode.com/problems/palindrome-pairs/solutions/2585494/c-trie-related-problems/

- jplank 3 is just the 'b+a version' of the following
- case 1 is aB
- case 2 is Ab
- oh why this is obviously correct:
	- for current word `a`, you grab all solutions where it comes first in the ordered pair
	- #/fundy  maybe this should be the standard approach to handling solutions where order matters
		- specifically, for ordered pairs:
			- during iteration i, you limit yourself to only computing solutions of the form (i,j) and not (j,i)
		- bc now it's really obvious that:
			- you get all solutions
			- you don't have to worry about duplicate solutions coming from other iterations
		- for this problem using this approach, the only time you need to worry about duplicates:
			- is during the current iteration
			- ie where neither a and b are split
				- aka you take the full word for both
				- ie b is reverse of a
				- bc this solution: satisfies both aB and Ab
- only need 1 trie:
	- store reversed words
	- store palindrome info (used for aB)
		- requires palindrome checks (on prefixes) when inserting words
	- store full word info (used for Ab)
		- when we reach 'full word' node, this induces a palindrome check on suffix of A
- possibly, but imo unecessary if you construct trie 'fully':
	- separate subroutine for the 'empty string' case


- i highly suspect: a big benefit of this approach is not having to construct a trie twice
	- it's an expensive operation and also lots of memory
	- same complexity, but better constant
	- the con is of 'you do more palindrome checks than necessary': seems it becomes irrelevant if you use modified kmp


## gallivan (hash)
- https://dev.to/seanpgallivan/solution-palindrome-pairs-23j6
- https://leetcode.com/problems/palindrome-pairs/solutions/1269310/js-python-java-c-easy-map-matching-solution-w-explanation/
	- look at comments too
- ok, gallivan’s solution now makes me see why grayson’s can be considered natural as well
- basically, by addressing the ‘obvious’ cases out of the way:
	- then the main routine (ie third case) can handle palindromes that involve nontrivial splits
	- and there should be a clean/easy/obvious way to see why this also handles duplicates without needing a nonsymmetric routines for the nontrivial splits
	- so maybe there’s a fundy here…bc lots of other problems, it’s true that the clean way to approach it, is to just handle all the easy cases first, so then you can just think about the ‘hard stuff’


## grayson (hash; duplicates/cases variant)
- another way to handle duplicates??
	- https://leetcode.com/problems/palindrome-pairs/solutions/2585442/intuitive-python3-hashmap-95-time-space-o-n-w-2/
		- this is different, bc observe: the third subroutine (aka the one that doesnt handle edge cases) has ‘symmetric/mirror’ treatment for the two subsubroutines
			- versus other hashmap solution examples, you typically see that the second subsubroutine deals with trivial splits
		- he has 3 solution subroutines:
			- the first two handle 2 types of ‘trivial’ splits (aka no splits)
				- the word’s reverse exists
				- the word is nontrivial and a palindrome, and the empty string exists
					- add both orders bc this routine skips solutions when empty string is current iteration
				- and it’s easy to see why these get all such solutions without duplicates
			- third routine
				- unlike other examples: his prefix/suffix range is k, not k+1
					- (where k is length of word)
					- i suspect that this, combined with the first 2 subroutines handling some (but not all) types of ‘trivial’ splits, is what allows him to someone how handle all trivial splits despite ‘symmetric’ routines
				- both subroutines are splitting the current word
					- so they’re finding stuff among Ab and bA
				- from the solutions he appends, (ie order of indices) we can see that
					- subsubroutine 1 is bA
					- subsubroutine 2 is Ab
				- subsubroutine 1:
					- all suffixes that aren’t empty
						- look for reversed suffix
						- palindrome check on the remaining prefix
					- so we still have trivial ‘split’ where we take the entire word as a suffix
						- ??? so why wouldnt this overlap with subroutine 1?
						- OHHHH…subroutine 1, it only finds a+b where:
							- a is not split
				- subsubroutine 2:
					- all prefixes that aren’t full word
						- look for reversed prefix
						- palindrome check on remaining suffix
					- so we have trivial ‘split’ where we take empty string as prefix
						- ??? why wouldnt this overlap with subroutine 2?



		#/todo full understand what’s going on here…???
see the comments:
	https://leetcode.com/problems/palindrome-pairs/solutions/2585442/intuitive-python3-hashmap-95-time-space-o-n-w-2/comments/1606119

(imo first understand gallivan above, he seems to divide into cases similarly)
(i think when i fully understand their cases approach, then it’ll be obvious why duplicate handling is so simple…ie don’t even have to explicitly handle it)



### similar (unintentional) duplicate handling
- https://leetcode.com/problems/palindrome-pairs/solutions/2585819/left-mid-right-palindrome-easy-to-understand-c-code/
	- this guy only does 2 subroutines, and he also has symmetric/mirror subsubroutines for the main subroutine
	- the subsubroutines seem to be leaving out one of the ‘trivial’ prefixes/suffixes bc his iteration also only goes to k
		- so i suspect its same thing as grayson



### (Ab + bA, 1 trie/pass) also another 'unintentional' duplicate handling, aka also works for trie solutions as expected
- https://leetcode.com/problems/palindrome-pairs/solutions/5118281/trie-solution-beginner-friendly/
	- also seems to not 'iterate' all the way
	- based on his 'insert' and 'search' functions:
		- he builds a trie of normal words; and traverses it backwards
	- his actual subroutines:
		- first one checks if prefix is palindrome, and then looks for reversed suffix in trie
			- so it is bA
		- second one checks if suffix is pal, then looks for reversed prefix in trie:
			- is Ab
- also: this is an example that Ab + bA strat, can also be done with just 1 trie/pass






## check for solutions (i,i)
- ie make sure that your routines throw away solutions (i,i)
	- (because the problem description says to ignore this)
	- if `a` is a palindrome, then `a+a` will be a palindrome, so you need to explicitly handle this case


## add a set solution

- https://leetcode.com/problems/palindrome-pairs/solutions/79209/accepted-python-solution-with-explanation/comments/765323
	- i should definitely have a solution where i just use set to handle duplicates
	- because avoiding duplicates is actually really subtle/nontrivial to explain
	- set approach won’t increase complexity, bc we already have O(n) time and space
		- but is a worse constant, because you have to go back and turn the set into a list


## btw, the 'left + mid + right' is a key idea
- the is basically the first/major observations that leads to the hash/trie solutions

## lc forum strats
- https://leetcode.com/problems/palindrome-pairs/solutions/1651095/O(nk)-solution-using-KMP-algorithm-for-finding-palindrome.-Analyses-of-all-algorithm/
	- see 3 and 4
		- kmp optimizations for palindrome checks



## complexity
- let n = num words and k = avg length of word
- supposedly:
	- brute: O(n^2 k)
	- hash: O(n k^2)
	- trie: O(n k^2)
- explanation
	- each solution needs at least a O(k) for the palindrome check
		- and i suspect lots of ppl are assuming we’re doing the palindrome check optimization..ie checking all pref/suffixes for palindromes is O(k), not O(k^2)

---


Both building and searching in the Trie structure take O(n * k^2). Here n is the number of words in vector and k is the average length of each word in vector


---

seems i need to look into how fast hashing actually is…
#hashing 



I think it took O(n)_O(k)_(O(m)+O(n)), and m+n is equal to k, so it took O(nk^2),  
O(n): loop the vector  
O(k): split string into two parts  
O(m): m is length of one part, if key is string, hash map take O(m) to find it.  
O(n): n is length of another part, determine if it is palindrome

- why in the above, finding hash is O(m) and not O(1):
	- “yes it took O(1) to find a key, but doesn't it need O(k) to do the hash?”
	- You are also right to consider that hashmap takes O(m) to find the string. Many beginners assume hash is O(1) for strings




## does python have a ‘string view’?
- if you code these solutions in c++, seems one major optimization is to use `string_view` 
	- https://stackoverflow.com/questions/40127965/how-exactly-is-stdstring-view-faster-than-const-stdstring
	- essentially, in this scenario, means you’re not constantly making copies of substrings
- does python have an equivalent? and does it matter?
	- probly should understand string slicing
		- https://realpython.com/python-strings/#string-slicing
			- doesn’t seem to mention anything
	- seems i need to use #python/memoryview
		- references
			- applicable (can’t find any yet)
			- these don’t seem to mention memoryview, but might be helpful to understand why string slicing is by copy
				- https://realpython.com/pointers-in-python/
				- https://realpython.com/python-memory-management/
			- unhelpful atm bc i’m too unfamiliar
				- https://docs.python.org/3/c-api/memoryview.html
				- https://docs.python.org/3/library/stdtypes.html#memoryview
		- discussions
			- string slicing is by copy
				- https://stackoverflow.com/questions/5722006/does-python-do-slice-by-reference-on-strings
				- https://stackoverflow.com/questions/64871329/does-string-slicing-perform-copy-in-memory
				- https://stackoverflow.com/questions/27013372/python-str-view
			- why memoryview
				- https://stackoverflow.com/questions/18655648/what-exactly-is-the-point-of-memoryview-in-python
				- 


## seems i should discuss python slicing syntax
- i guess this was obvious af to me bc it’s necessary in math/academia/numpy

- importantly (i didn’t even realize this):
	- seems ‘step’ gets processed first, ie why:
		- https://leetcode.com/problems/palindrome-pairs/solutions/2585442/intuitive-python3-hashmap-95-time-space-o-n-w-2/comments/1607151



#python/slicing
- usually, it’s (start, stop, step)
- so if you set `step=-1` then you get a reversed slice




## ramblings about efficiency
- it seems hashmap is more efficient in practice
- i need to compute the asymptotic complexity
- if hashmap is indeed worse theoretically but more efficient in practice, i need to understand why
	- possibly bc ‘traversing a trie’ happens in ‘python loops’…
		- aka the hashmap approach is just minimizing how many ‘python loops’ we have to do, and that beats out the ‘theoretical better complexity’
	- also/or: could just be the nature of the test cases




- #/research remember to check other solutions
	- https://leetcode.com/problems/palindrome-pairs/submissions/1264297326/

# References


## #hashing/collisions 

- the problems statement says that all strings are unique
	- that's why we can just associated a unique index to each string
	- 


- i should probably point out that a classic (or the basic) use case of this technique is when we have:
	- a list of elements
	- some kind of function such that
		- f(x) = y IFF f(y) = f(x)
		- aka defines a unique ‘pair’ for each element
	- a question that is of any of the following forms:
		- does a pair exist
		- count pairs
- and so then the classic strat is:
	- iterate over elements once
	- as we iterate: store visited stuff in hashmap
	- so then future elements: can efficiently determine if it’s “pair” exists


- this problem is a variant + significantly increases the difficulty bc:
	- each element has multiple collision checks
		- there’s basically only 1 way to do this: essentially
			- for each word: for each prefix and suffix:
				- check collisions on the reverse:
				- (and other conditions)
			- observe that this space complexity won’t be that bad, bc the above strat only requires storing the whole word as a collision
				- ie, we don’t store all prefixes/suffixes as collisions
					- importantly: this wouldn’t actually work or make any sense
					- bc this means your main routine is like: you’re given a,
					- and you want to figure out if a word p+ra exists where p is a palindrome and ra is reverse of a
					- so this makes zero sense, bc how could you ever search for p+ra efficiently




## #interview/clarify

- this seems like another example where asking questions is important
- the constraints seem allow empty string
- observe the constraints do not allow an ‘empty string’
	- if this was allowed, it would add noticeable more work to the problem
- wait i’m so confused…one of the test cases has an empty string…


# Strategies


## (only) hashmap

- examples
	- https://algo.monster/liteproblems/336


#/code 
- i’m not a fan of the code below
	- imo use better variables names…particuarly, imo i think i should let `mid` be substring where i run a palindrome check?
	- ehh actually, the code below is honestly decent, once you understand the strat..i think it’s enough to have some comments be like:
		- `# check palindrome a + b + ra`
		- and `# check palindrome rb + a + b`


```python
# leetcode.ca
class Solution:
    def palindromePairs(self, words: List[str]) -> List[List[int]]:
        d = {w: i for i, w in enumerate(words)}
        ans = []
        for i, w in enumerate(words):
            for j in range(len(w) + 1):
                a, b = w[:j], w[j:]
                ra, rb = a[::-1], b[::-1]
                if ra in d and d[ra] != i and b == rb:
                    ans.append([i, d[ra]])
                if j and rb in d and d[rb] != i and a == ra:
                    ans.append([d[rb], i])
        return ans

# lcforums

```

``



## probly hashmap

```python
# walkccc.me
class Solution:
  def palindromePairs(self, words: List[str]) -> List[List[int]]:
    ans = []
    dict = {word[::-1]: i for i, word in enumerate(words)}

    for i, word in enumerate(words):
      if "" in dict and dict[""] != i and word == word[::-1]:
        ans.append([i, dict[""]])

      for j in range(1, len(word) + 1):
        l = word[:j]
        r = word[j:]
        if l in dict and dict[l] != i and r == r[::-1]:
          ans.append([i, dict[l]])
        if r in dict and dict[r] != i and l == l[::-1]:
          ans.append([dict[r], i])

    return ans
```



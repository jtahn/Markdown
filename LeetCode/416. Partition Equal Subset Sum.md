[416. Partition Equal Subset Sum](https://leetcode.com/problems/partition-equal-subset-sum/)

```python
class Solution:
    def canPartition(self, nums: List[int]) -> bool:
        
```

# Description

Given an integer array `nums`, return `true` _if you can partition the array into two subsets such that the sum of the elements in both subsets is equal or_ `false` _otherwise_.

**Example 1:**  
**Input:** `nums = [1,5,11,5]`  
**Output:** `true`  
**Explanation:** The array can be partitioned as `[1, 5, 5]` and `[11]`.

**Example 2:**  
**Input:** `nums = [1,2,3,5]`  
**Output:** `false`  
**Explanation:** The array cannot be partitioned into equal sum subsets.

**Constraints:**
- `1 <= nums.length <= 200`
- `1 <= nums[i] <= 100`

---



# todo

#/move 

- great starter resources for codeforces research
	- [I compiled a list of almost all useful blogs ever published on Codeforces [update: till 09.06.2021] - Codeforces](https://codeforces.com/blog/entry/91363)
	- [All the good tutorials found for Competitive Programming - Codeforces](https://codeforces.com/blog/entry/57282)
	- [Good Blog Post Resources about Algorithm and Data Structures - Codeforces](https://codeforces.com/blog/entry/13529)
	- [Top 10 best Codeforces blog posts of 2021 (in my humble opinion) - Codeforces](https://codeforces.com/blog/entry/98250)
	- [Codeforces Popularity Ranking - Codeforces](https://codeforces.com/blog/entry/61375)


- meta posts
	- [Perhaps you should give up - Codeforces](https://codeforces.com/blog/entry/118674)
	- [How to Interpret Contest Ratings - Codeforces](https://codeforces.com/blog/entry/68288)


- look for fundies/problems:
	- https://en.wikipedia.org/wiki/Subset_sum_problem


# References

## #dynamic_programming 
- move these to the fundy
	- (first see if my refs or cmu have explanations of this)
		- then go through these links and see if there's anything extra i want to look for
		- examples cmu stuff:
			- [lecture24.pdf](https://www.cs.cmu.edu/afs/cs/academic/class/15210-s12/www/lectures/lecture24.pdf)
			- [dp-notes.pdf](https://www.cs.cmu.edu/afs/cs/academic/class/15210-s15/www/lectures/dp-notes.pdf)
			- [lec09-dp1.pdf](https://www.cs.cmu.edu/~15451-s23/lectures/lec09-dp1.pdf)
	- if dp is possible, then both top-down and bottom-up strats exist
		- [algorithm - Does there always exist a dynamic programming bottom up solution for corresponding memoization method - Stack Overflow](https://stackoverflow.com/questions/29145517/does-there-always-exist-a-dynamic-programming-bottom-up-solution-for-correspondi)
	- bottom-up is generally better
		- [recursion - Dynamic programing: Tabular vs memoization - Stack Overflow](https://stackoverflow.com/questions/70858465/dynamic-programing-tabular-vs-memoization)
		- [terminology - Dynamic Programming vs Memoization - Computer Science Stack Exchange](https://cs.stackexchange.com/questions/99513/dynamic-programming-vs-memoization)
		- [When to use bottom up DP and top down DP? : leetcode](https://www.reddit.com/r/leetcode/comments/164tz2c/when_to_use_bottom_up_dp_and_top_down_dp/)
		- [How to decide whether to use tabulation or memoization for DP problems? : csMajors](https://www.reddit.com/r/csMajors/comments/rsfp3o/how_to_decide_whether_to_use_tabulation_or/)
		- [algorithm - Is Bottom-up DP solution better than Top-down in terms of Time complexity? - Stack Overflow](https://stackoverflow.com/questions/62629547/is-bottom-up-dp-solution-better-than-top-down-in-terms-of-time-complexity)
		- [algorithm - when to use bottom-up DP and when to use top-down DP - Stack Overflow](https://stackoverflow.com/questions/34897484/when-to-use-bottom-up-dp-and-when-to-use-top-down-dp)
		- [Dynamic programming is not black magic | Hacker News](https://news.ycombinator.com/item?id=38988948)
			- The bottom-up tabularization isn't always necessary, but it's a key technique that is not just "avoiding overhead" or whatever. Doing the top-down memoization, you don't know which subproblems you need to store at any given, but going bottom-up, you do, and you can usually be much more clever about memory usage and overhead. The obvious example is Fibonacci numbers: the "top down" memoized version needs to store every Fibonacci number in memory, because you don't know what number the recursion is asking for. But if you do it "bottom up", you know that to calculate f(N), you only need f(N-2) and f(N-1). Therefore, you only need to store the last two numbers, reducing your memory usage from O(n) to O(1). This principle almost always applies in DP problems: for the "edit distance" problem, the number possible values for the function is N^2 (where N is the size of the two words you need to compare, assuming they're similar sizes), so the memoized version takes O(N^2) memory. But if you do it the "bottom up" version, you realize that to calculate all values in a row, you just need the previous row of values. So it takes memory from O(N^2) to O(N). Point being: turning the recursion "upside down" is not just a thing you do to be clever (or to avoid "function calling overhead", something like that), it has very real algorithmic benefits, usually because it takes MUCH less memory to do it like that.
			- The point is that you don't have to exactly fill the 2D array to solve the problem in that way. The 2D array is an optimization in this view, and can be safely replaced with a cache without breaking the correctness. Of course there is also some learned techniques specific to dynamic programming, and that makes it worthy to learn at some point because otherwise you will never think of them, but at its core dynamic programming is just a specific way of doing recursion.
			- 1. *Fibonacci Sequence*: The classic example where the naive recursive solution has exponential complexity. By storing previously computed values (memoization), the complexity can be reduced to linear. 2. *Coin Change Problem*: Given different denominations of coins and a total amount, finding the number of ways to make the change. The naive approach is exponential, but dynamic programming reduces it to polynomial complexity. 3. *Knapsack Problem*: Particularly the 0/1 Knapsack problem, where items with given weights and values must be placed in a knapsack of a fixed capacity to maximize total value. The naive exponential solution can be optimized using dynamic programming. 4. *Matrix Chain Multiplication*: Determining the most efficient way to multiply a chain of matrices. The problem can be solved in exponential time using a naive approach but becomes much more efficient with dynamic programming. 5. *Longest Common Subsequence*: Finding the longest subsequence common to two sequences. A classic dynamic programming problem that can be solved in polynomial time. 6. *Longest Increasing Subsequence*: Finding the length of the longest subsequence of a given sequence such that all elements of the subsequence are sorted in increasing order. 7. *Shortest Path Problems*: Like the Floyd-Warshall algorithm for finding the shortest paths in a weighted graph with positive or negative edge weights. 8. *Edit Distance (Levenshtein Distance)*: Finding the minimum number of edits (insertions, deletions, substitutions) needed to change one word into another.




# Results


## #complexity : Pseudo-polynomial time

- definition:
	- https://en.wikipedia.org/wiki/Pseudo-polynomial_time
		- In computational complexity theory, a numeric algorithm runs in pseudo-polynomial time if its running time is a polynomial in the numeric value of the input (the largest integer present in the input)—but not necessarily in the length of the input (the number of bits required to represent it), which is the case for polynomial time algorithms.
		- In general, the numeric value of the input is exponential in the input length, which is why a pseudo-polynomial time algorithm does not necessarily run in polynomial time with respect to the input length.



## #/tbd

- be aware that this is a classic problem
	- https://en.wikipedia.org/wiki/Partition_problem


## #/tbd 
- general idea behind:
	- to find partitions with equal sum:
		- equivalent to: find a subset that achieves half
		- and this equivalent formulation seems far easier/direct to solve
- attempts to generalize:
	- instead of testing equality btwn 'things that can vary'
		- can we test equality with something fixed
	- if we want to split into 2 wrt some condition:
		- can we instead come up with a condition for a subset
		- ie use the fact that this probly results in a symmetric condition for the complement of subset




## #/tbd 

- see discussion in strat below
- it seems in dp, you won't always be able to easily formulate previous data as 'solutions to subproblems'
	- aka probly, i should eventually realize that dp is more about caching/saving intermediate data
		- (if indeed, this is what skiena/CLRS say as well)
- aka here:
	- the stuff we store is just about 'what sums have we found so far'
	- otoh...i suppose you could formulate this as a problem:
		- ie see the wikipedia link
		- p(i,j) is the problem
		- 



## #/tbd 
- neetcode using set; versus algomonster using array
	- there's a 'flipping what we store' idea going on
- see discussion in strat below





# Strategies



## dynamic programming

- implementation 1
	- https://en.wikipedia.org/wiki/Pseudopolynomial_time_number_partitioning#Recurrence_relation
		- this solution is strictly worse space than the one below
			- you can optimize it a bunch since p(i,j) only needs stuff on level j-1 and j
				- you definitely could do it with just level j-1 and build level j
				- you could even probly modify level j-1 in place to get level j
		- best time compared to other algos on wiki, see other strat headings; poor space
			- it's only reasonable here bc of constraints on num length and numerica value
			- time
				- O(K/2 N), where N is the number of elements in the input set and K is the sum of elements in the input set.
			- space
				-  takes O(nm) memory, where m is the largest number in the input.

		- i honestly have no clue why this solution uses so much space, bc the optimization to use 1d space should be very obvious
			- i suspect: the wiki article just copy pasted an algo that is meant to solve a much more difficult problem (and so might require the 2d space)


- the central idea behind dynamic programming strat is that:
	- we observe size of 'intermediate sums that are worth keeping around' is small
		- for this problem, target is small and numbers are all positive.
	- and since we only care if the split is possible:
		- it doesn't actually matter that there's 2^n ways to make subsets out of n elements
		- bc all we care about is the value of the output
			- which only has a limited number of possibilities



- implementation 2
	- https://algo.monster/liteproblems/416
		- ohhh actually, this is the optimization to implementation 1 that i was thinking of
	- neetcode:
		- starting around 8:00
		- https://www.youtube.com/watch?v=IsvocB5BJhw&list=PLPe9IkX86X3y5m_MvtNu2ughxsvkqUNKr&index=110
		- he does the annoying thing where he iterates backwards, and seems unecessary
		- this isnt the same implementation as the wiki article, but it's same strat
			- difference:
				- wiki article:
					- structure that we iteratively fill up to determine what sums are (im)possible using currently encountered numbers
				- neetcode:
					- structure that we fill up with sums encountered so far
		- imo: this difference in implementation does use another important key idea
			- so maybe it should be a diff strat
			- bc this strat is not:
				- solutions to previous subproblems
			- instead this strat is:
				- ???
- this strat also...we're not even really solving subproblems tbh
	- we're just caching/tabling stuff
	- we're looking at subsets and caching the possible sums
		- ie not actually solving subproblems
	- and then all the neetcode implementation is doing:
		- caching possible sums: it's sufficient to use a set
			- ie don't need to use an array like algomonster (1d optimization over wikipedia)

- i like algomonster bc it makes it very obvious to me why this is dynamic programming
	- neetcode using a set: it's same idea, but just 'hides' it a bit
		- and not necessarily 'better'
		- probly simpler to code tho
		- actually neetcode is better constant probly
			- bc u just iterate over sums you already have
			- versus algomonster iterates over all possible sums up through target
		- ok so neetcode idea is just like:
			- if we care about which sums are possible:
				- can just 'flip' the problem: what sums have we found





```python
# cache/table via set
class Solution:
    def canPartition(self, nums: List[int]) -> bool:
        if sum(nums) % 2:
            return False

        dp = set()
        dp.add(0)
        target = sum(nums) // 2

        for i in range(len(nums) - 1, -1, -1):
            nextDP = set()
            for t in dp:
                if (t + nums[i]) == target:
                    return True
                nextDP.add(t + nums[i])
                nextDP.add(t)
            dp = nextDP
        return False


```





## others
- https://en.wikipedia.org/wiki/Partition_problem#Exact_algorithms
	- O(n) space, O(n^2) time
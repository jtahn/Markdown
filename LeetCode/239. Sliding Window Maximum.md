[239. Sliding Window Maximum](https://leetcode.com/problems/sliding-window-maximum/)

You are given an array of integers `nums`, there is a sliding window of size `k` which is moving from the very left of the array to the very right. You can only see the `k` numbers in the window. Each time the sliding window moves right by one position.

Return _the max sliding window_.

**Example 1:**  
**Input:** `nums = [1,3,-1,-3,5,3,6,7], k = 3`  
**Output:** `[3,3,5,5,6,7]`  
**Explanation:**  
```
Window position                Max
---------------               -----
[1  3  -1] -3  5  3  6  7       **3**
 1 [3  -1  -3] 5  3  6  7       **3**
 1  3 [-1  -3  5] 3  6  7      ** 5**
 1  3  -1 [-3  5  3] 6  7       **5**
 1  3  -1  -3 [5  3  6] 7       **6**
 1  3  -1  -3  5 [3  6  7]      **7**
```

**Example 2:**  
**Input:** `nums = [1], k = 1`  
**Output:** `[1]`  

**Constraints:**
- `1 <= nums.length <= 10^5`
- `-10^4 <= nums[i] <= 10^4`
- `1 <= k <= nums.length`

---

# todo
- note that [42. Trapping Rain Water](42.%20Trapping%20Rain%20Water.md) also has both monotone stack and dynamic programming as optimal solutions
	- if you consider 'dynamic programming' = 'two pointer'...
	- what is simiilar/different about these problems
	- what is siimlar/different about these techniques
- language here should be reduced a lot
- max heap implementation
	- i think my explanation of complexity is incorrect
		- pop is just amortized O(1) right?
		- i think the issue is the push operation, this is O(log n); so this is why ttl time is O(n log n)
	- p sure i can just store indices
		- to check the value associated with the index, I can just check the array
		- so then the 'trick' involved here is just like all the other solutions...ie the big idea is that you need to be storing the indices (but can add on values too, if you care less about space)
			- why you should realize this trick/key idea:
				- bc left pointer, might need to ignore info from it
	- go confirm that in python heapq, it accepts like a 'key comparison function', that way the heapq can just store the indices
		- so for this problem: lambda function would access array and inverts the value
		- if it doesn't: then yea, i'll need to do that trick where i store tuples of index and value
		- actually, this trick should then go into the fundamental problem for max heap 
			- [703. Kth Largest Element in a Stream](703.%20Kth%20Largest%20Element%20in%20a%20Stream.md)
				- hmm this trick might be too simple for this problem...might need a diff fundamental problem for this (ie where keys that you use to sort, are diff from actual 'records')
	- my point is that: the key idea here is not about storing tuples; its about the fact that, the stuff you need to store in your structure is the index
		- ie this is the idea that carries over to other languages; or when implementing your own custom heap class
		- and storing tuples vs using key comparison function: asymptotically the same. so this kinda tells you that, 'storing tuples' is not the key idea bc there's basically an equivalent approach...but the idea of 'storing index' is a key idea, bc an approach that doesn't do this...very likely strictly worse asymptotically
- max heap
	- i'm p sure: popping the left pointer is O(1) amortized
		- bc you'd only delete if it's the max
		- otherwise, it's fine if it's elsewhere in the heap
		- ie you never actually waste time trying to find it
- find the fundamental problem where self-balancing bst is the optimal solution
	- likely max-heap would also work there, but would be worse
	- so then the discussion i have here about how these approaches differ and tradeoffs, should be moved to that problem
		- check clrs
			- surely there's discussions about max heap vs self-balancing bst
				- ie understanding tradeoffs/complexity
		- ie: in what ways is max-heap worse than self-balancing BST?
			- check the push op...iirc BST is log(k) but heap is log(n)
			- which would mean bst is like: asymptotic tradeoffs:
				- slower to compute max
				- faster to add new nodes to structure
			- so then tradeoffs for implementation: depends on what you expect your data to look like...
				- find specific simple examples for worst case of heap...so then basically, that thing you wouldnt use heap; but if its the 'opposite', then maybe you should use heap
- add global observation
	- whatever structure you use
		- will likely order stuff based on value, to make it more efficient to compute max
		- but it needs to allow the index to be easily accessible; ie indices need to be stored
			- need to verify that values outside the window aren't interfering with your max computation
	- aka there's a distinction here btwn: what you are storing vs what you are ordering by








# Brute force
- for each window, compute max in the window
- complexity
	- O(nk) time
		- for each n-k elements, scan k elements to find max of the window

# max heap
```
from heapq import *

class Solution:
    def maxSlidingWindow(self, nums: List[int], k: int) -> List[int]:
        
        H = []
        ans = []
        
        for i in range(k):
            heappush(H,[-nums[i],i])
        
        ans.append(-H[0][0])
        
        for i in range(k,len(nums)):
            heappush(H,[-nums[i],i])
            
            while H and i - H[0][1] >= k:
                heappop(H)
            
            ans.append(-H[0][0])
        
        return ans
```

- one bottleneck in brute force, is the computation of the max
	- is there a way to do this faster?
	- fastest would be max heap
		- but issue with max heap: how do we delete the left pointer stuff
- if your structure exactly contains elements in the window, then that means every time window shifts, you need to figure out where the left node is
	- (so our structure is size k)
	- max heap is not good for this
		- finding/deleting is O(k)
	- binary search tree is better for this
	- so that's why observation:
		- maybe allow structure to not exactly contain elements in the window?
			- deque: might store less; see later
			- better max heap: will store more elements than needed; read on...
- observation
	- issue with heap is that: not rly an efficient way to delete the node that's exiting the window
		- way to get around this: observe we only have to delete the node if it the max, otherwise nbd if it's still there
		- so the solution: store not just the value, but the index: so we can quickly check if max value is outside window
	- note: it is NOT true that search/delete is O(log n), despite 'binary' heap
		- !! binary heap and binary search tree have DIFFERENT inequality relations
			- heap property: parent greater than children
				- this tells you nothing about where your node is
				- literally all it is helpful for is finding the max, that's literally it
			- binary search tree: left child <= parent <= right child
				- this relationship is what leads to O(log n) search/delete
- seems that generally, heaps are assumed to be binary heaps
	- ie python implementation is binary heap
	- wiki says common implementation is binary heap
- implementation
	- the while loop: pops max elements that are invalid; ie index is out of the window
		- index of newest node is `i`
		- max node is `H[0]`
			- so `H[0][1]` is its index in the array
		- so `i - H[0][1] >= k` checks whether the current max value is actually outside the window
		- if so, we pop it
			- and we keep doing this as needed
		- once while loop ends, we know the current heap max is in the window
- complexity
	- time O(n log n)
		- note maxheap has no required max size; it only pops if top is outside bounds, but there's no reason this needs to happen
		- ie we'd get a maxheap with the entire sequence if it is an increasing sequence
			- bc we never need to pop; bc the max is always in the window
		- so worst case assume maxheap is size n
			- in which case, operations are O(log n)
	- space
		- O(n)
			- note the constant is worse than other methods below, bc you essentially store 2 piece of info for every node
			- other methods, you can just store either index or the value; here, you need to store both



# Self-balancing BST
```
class Solution {
    public int[] maxSlidingWindow(int[] arr, int k) {
        int n = arr.length, j = 0;
        int[] ans = new int[n - k + 1];
        TreeMap<Integer, Integer> bst = new TreeMap<>();
        for (int i = 0; i < n; i++) {
            bst.put(arr[i], bst.getOrDefault(arr[i], 0) + 1);
            if (i + 1 >= k) {
                ans[j++] = bst.lastKey(); // return max element in BST
                removeElement(bst, arr[i+1-k]);
            }
        }
        return ans;
    }
    void removeElement(TreeMap<Integer, Integer> bst, int x) {
        bst.put(x, bst.getOrDefault(x, 0) - 1);
        if (bst.get(x) == 0) bst.remove(x);
    }
}
```

- todo
	- apparently above code only works if the elements in k window are unique as TreeMap is based on Red-Black tree which can't have duplicates
		- what do you do if duplicates?
	- what does self balancing mean
- strategy
	- add right pointer value to bst
	- (find and) delete left pointer value from bst
		- we know value from array
	- find max in bst
- complexity
	- time O(N log k)
		- because each of the above operations of a BST of size k costs O(log k)
	- space O(k)
- observations
	- basically, this is like a balance btwn 'quickly find max' and 'quickly find node to delete'
		- we sacrifice O(1) computation of max, so that "deleting node" is gauranteed O(log n)


# Decreasing deque (best)
```
from collections import deque

"""cleanest"""
class Solution:
    def maxSlidingWindow(self, nums, k):
        deq, n, ans = deque([0]), len(nums), []

        for i in range (n):
            while deq and deq[0] <= i - k:
                deq.popleft()
            while deq and nums[i] >= nums[deq[-1]] :
                deq.pop()
            deq.append(i)
            
            ans.append(nums[deq[0]])

		# note the first k entries in ans correspond to
		# maxes of windows that are smaller than size k
		# because of how we looped above
        return ans[k-1:]        


"""shorter"""
class Solution:
    def maxSlidingWindow(self, nums: List[int], k: int) -> List[int]:
        q = deque() # stores *indices*
        res = []
        for i, cur in enumerate(nums):
            while q and nums[q[-1]] <= cur:
                q.pop()
            q.append(i)
            # remove first element if it's outside the window
            if q[0] == i - k:
                q.popleft()
            # if window has k elements add to results (first k-1 windows have < k elements because we start from empty window and add 1 element each iteration)
            if i >= k - 1:
                res.append(nums[q[0]])
        return res
```

![](../!assets/attachments/Pasted%20image%2020240306122140.png)




- one of the observations here
	- assume window is sliding left to right
		- then incoming number is from the right
		- then the structure does not need to keep all window elements
			- if the incoming number is larger than prev numbers; then because of the way we're iterating/analyzing/sliding the window, we know those previous numbers have no chance of being the max in the current and future windows
		- more generally: its because: for all future windows to be scanned, if the 'previous numbers' are there, then so will the incoming number
- jargon that is typically used for this
	- monotonic (increasing) queue
	- decreasing deque
		- a deque is a data structure that allows insertion and deletion from both the front and the back in `O(1)` time complexity
- Sliding window minimum/maximum = monotonic queue
	- The key why monotonic deque works is it stores both magnitude and position information. From head to tail, the elements get smaller and further to the right of the array.
- monotonic queue involves 3 operations
	- pop; aka discard left pointer
		- the only index that might corresp to index of left pointer, is exactly the left-most thing in the deque
			- so we check if its the left pointer; if not, there's nothing to discard/pop
	- push: add right pointer
		- not only do we add it; we also discard non maxes in the window
			- aka anything in the deque that is smaller than the right pointer
		- ie we also discard stuff that is smaller
		- ie keep popping from the right if smaller than right pointer
		- this process is O(1) amortised
	- peek
		- aka tell us the max
		- deque ordered by max
		- which means, left pointer is also the index of max in the window
	- importantly
		- these 3 operations are all O(1)
		- these are the only ops we need for this problem
	- apparently: using a monotonic queue has a very established sequence for each iteration: can either:
		- push,peek,pop
		- pop,push,peek
		- pop,
		- they're all the same..only diff, slight modification to stuff before and after the loop...
- complexity
	- O(n) time
		- since each element is processed (add/remove) at most twice.
		-  The algorithm is amortized O(n) as each element is put and polled once.
		- each element in the original array can only be pushed into and popped out of the deque only once.
	- O(k) space



- In the deque, we add and remove indices.
	- Basically, for each element `nums[i]` in the array that we are about to insert, we first check whether the leftmost index in the sliding window is out of bound. If so, we remove it in constant time.
	- Then we continuously remove the rightmost indices if their corresponding elements are less than `nums[i]` (the element we are about to insert). The idea is that the elements that are less than the element we'll insert won't have any contributions to the maximum element of the sliding window. So it is safe to remove them.
- monotonic deque properties:
	- elements are in decreasing order
- push
	- to enforce monotonic property: means that to push an element into the deque, we first pop everything smaller than it out of the deque


- deque properties/rules:
	- decreasing order
	- only contain elements from the last sliding window (not necessary all elements)
	- the above two imply that:
		- largest element in sliding window is the first element in deque
	- summary
		- Keep indexes of good candidates in deque d. The indexes in d are from the current window, they're increasing, and their corresponding nums are decreasing. Then the first deque element is the index of the largest window value.
- better names
	- right pointer = incoming number
- strategy
	- deque lets us pop from the left O(1) for when a number no longer within, aka exits our sliding window
	- after the pop: the left most value is the largest value in the window
- space complexity
	- O(k) for the deque
	- O(n-k) for output array


- errors
	- forgetting `while q ...`
		- ie forgetting to check the queue exists/is not empty (to prevent popping from an empty q)
	- ![](../!assets/attachments/Pasted%20image%2020240306155855.png)


# Dynamic programming (also best)
```
def foo():

	left = []
	right = []
	maxes = []

	# compute left maxes
	for m in range(len(nums)):
		if m % w == 0:
			left[m] = nums[m]
		else:
			left[m] = max(left[m-1], nums[m])

	# compute right maxes
	for m in range(len(nums)):
		if m % w == w-1:				# alternatively: (m+1) % w == 0:
			right[m] = nums[m]
		else:
			right[m] = max(right[m+1], nums[m])

	# compute window maxes
	for m in range(len(nums)-w):
		maxes[m] = max(right[m], left[m+w-1])

	return maxes
```




![](../!assets/attachments/Pasted%20image%2020240307082829.png)

- strategy
	- partition the array into blocks of width `w`, starting the left edge (right block can have width `<w`)
	- block coords:
		- indices of elements in the `i^th` block are `i*w + j` where `j = 0, ..., w-1`
			- ie `0 <= j < w`
			- so `j` is like the offset within a block
			- (todo: seems j isn't really needed...seems block coords arent needed...)
	- now consider the window that starts at index `m`
		- in terms of "block coords", this corresp to some `i*w+j`
		- first observation is that the max of this window is the max of two quantities:
			- max of the blue part
				- aka max to the right of the left pointer in that block
				- aka in `right[m]`
			- max to the yellow part
				- max to the left of the right pointer in that block
				- aka in `left[m+w-1]`
			- aka `d[m]=max(right[m], left[m+w-1])`
		- second observation
			- the arrays `left` and `right` can each be computed in one pass
			- note `left` is the 'accumulative max from left to right in each block'
				- so can be computed via passing left to right
- complexity
	- O(n) time
	- O(n) space
- errors
	- make sure last entry of array is computed via `d[n-w] = max(right[n-w], left[n-1])`
		- aka the last iteration should be on index `n-w`
- terminology
	- 'anchor points' are where the blocks meet



---


- prev explanation of strategy(probly delete)
	- one way to interpret this (but is not really an inspo):
		- max over an array can an array is equal to
			- max of maxes of subarrays, where the subarrays cover the array
				- (ie they dont have to be disjoint subarrays...but for efficiency, probly dont want to be computing maxes of disjoint subarrays)
		- partition array into windows of size k
			- then any window intersects with most 2 elements of the partition
				- ie the intersections are highlighted in blue and yellow in the picture
				- max of blue part
					- is also equal to "current max during leftward pass in that window"
				- max of yellow part
					- is also equal to "current max during rightward pass in that window"
		- so that's why we get
			- max of the window = max(  right(i), left(i+w-1)  )


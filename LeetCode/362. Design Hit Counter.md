[362. Design Hit Counter](https://leetcode.com/problems/design-hit-counter/)

```python
class HitCounter:
    def __init__(self):
	    

    def hit(self, timestamp: int) -> None:
	    

    def getHits(self, timestamp: int) -> int:
	    


# Your HitCounter object will be instantiated and called as such:
# obj = HitCounter()
# obj.hit(timestamp)
# param_2 = obj.getHits(timestamp)
```

# Description

Design a hit counter which counts the number of hits received in the past `5` minutes (i.e., the past `300` seconds).

Your system should accept a `timestamp` parameter (**in seconds** granularity), and you may assume that calls are being made to the system in chronological order (i.e., `timestamp` is monotonically increasing). Several hits may arrive roughly at the same time.

Implement the `HitCounter` class:
- `HitCounter()` Initializes the object of the hit counter system.
- `void hit(int timestamp)` Records a hit that happened at `timestamp` (**in seconds**). Several hits may happen at the same `timestamp`.
- `int getHits(int timestamp)` Returns the number of hits in the past 5 minutes from `timestamp` (i.e., the past `300` seconds).

**Example 1:**  
**Input:**
```
["HitCounter", "hit", "hit", "hit", "getHits", "hit", "getHits", "getHits"]
[[], [1], [2], [3], [4], [300], [300], [301]]
```
**Output:** `[null, null, null, null, 3, null, 4, 3]`  
**Explanation:**
```
HitCounter hitCounter = new HitCounter();
hitCounter.hit(1);       // hit at timestamp 1.
hitCounter.hit(2);       // hit at timestamp 2.
hitCounter.hit(3);       // hit at timestamp 3.
hitCounter.getHits(4);   // get hits at timestamp 4, return 3.
hitCounter.hit(300);     // hit at timestamp 300.
hitCounter.getHits(300); // get hits at timestamp 300, return 4.
hitCounter.getHits(301); // get hits at timestamp 301, return 3.
```

**Constraints:**
- `1 <= timestamp <= 2 * 10^9`
- All the calls are being made to the system in chronological order (i.e., `timestamp` is monotonically increasing).
- At most `300` calls will be made to `hit` and `getHits`.

**Follow up:** What if the number of hits per second could be huge? Does your design scale?

---

# todo

#/research
- at some point, consider looking at these vids?
	- not urgent tho, this solution is straightforward if you understand trie fundeez
	- [DESIGN HIT COUNTER | LEETCODE 362 | PYTHON BINARY SEARCH SOLUTION - YouTube](https://www.youtube.com/watch?v=MKihMUdG3O8)
	- [LeetCode 362. Design Hit Counter - YouTube](https://www.youtube.com/watch?v=WkLuQeVsXtY)

#/cite 
- check halim/epi/skiena/etc...one of them must have this problem + jargon


#/solutions 
- g4g actually seems fairly comprehensive?
	- ie for [[362. Design Hit Counter]]:
		- https://www.geeksforgeeks.org/design-a-hit-counter/
- https://github.com/grandyang/leetcode



#/meta 
- i haven't been going through the lc hints for each problem...should i start looking at those in later reviews?
	- maybe they could help with fundeez?


#/problems 
- 359 logger rate limiter
	- https://leetcode.com/problems/logger-rate-limiter/description/
	- https://leetcode.ca/2016-11-23-359-Logger-Rate-Limiter/
	- strat uses dict/counter
		- https://algo.monster/liteproblems/359
		- https://github.com/doocs/leetcode/tree/main/solution/0300-0399/0359.Logger%20Rate%20Limiter



#/meta 
- seems this is truly a #design problem
	- start with the simple case, regardless of concurrency/scalability/etc, and get a feasible solution first
	- there's no 'overall' best solution here...just depends on the situation


#python/lists
- time complexity of common operations 
	- len is O(1)
		- [Why is the complexity of len(list) O(1)? : learnpython](https://www.reddit.com/r/learnpython/comments/752zbp/why_is_the_complexity_of_lenlist_o1/)
		- [python - Cost of len() function - Stack Overflow](https://stackoverflow.com/questions/1115313/cost-of-len-function)
		- [c - What is the secret behind Python's len() builtin time complexity of O(1) - Stack Overflow](https://stackoverflow.com/questions/52134512/what-is-the-secret-behind-pythons-len-builtin-time-complexity-of-o1)
		- [Internal Working of the len() Function in Python - GeeksforGeeks](https://www.geeksforgeeks.org/internal-working-of-the-len-function-in-python/)
		- [Why len(list) has a time-complexity of O(1)? : leetcode](https://www.reddit.com/r/leetcode/comments/14ygdvq/why_lenlist_has_a_timecomplexity_of_o1/)
	
	- max/min is O(n)
		- cannot be implemented with O(1), unless you put restrictions on how the list is used
		- https://stackoverflow.com/questions/71684970/understanding-pythons-len-time-complexity
			- Keep in mind that lists aren't append-only; it would be trivial to track the min/max if they were, but once you open the possibility of invalidating the current maximum, things get more complicated (heaps are the standard solution to this, but do you really want every list to store a heapified copy of itself).
	- there's like a subtle concept here:
		- python list is more specific than 'array data structure from dsa textbooks'
			- partly cuz it supports these extra methods in o(1) time
		- i think there's something like: it guarantees the performance u expect from 'array structure standard operations' 
		- understanding when language designers decide to add 'efficient methods' to a structure, at the cost of slight inefficiencies elsewhere
			- ie, now the 'list' structure has to store a var corresp to it's length, and also update this every time we modify the list




#/jargon 
- all approaches are storing hits...but just in different ways
- what is the right way to differentiate btwn structure that is:
	- 'key hits': storing all hits separately
	- 'key times': storing count of hits, wrt the time




#/induction
- (i actually think this tag is needed now)
	- for when i don't feel like i understand the true structure/idea underlying the problem, or how strats are connected
		- #ad_hoc applies to like 1 strat...but sometimes, i know there's like a comparison btwn strats that's important to realize, and requires truly understanding the abstraction/structure of the problem
- maybe something like:
	- here: you have a choice btwn storing hits or 'counts of hits'
		- inherently, this is a tradeoff between:
			- time and space
			- bc: storing hits is worse space but better time
				- bc gethits is now just length of a (sub)array
			- but storing counts: better space but worse time
				- bc gethits now has to sum across the counts




# References





## #binary_search 

- strat
	- store each hit separately
	- never delete
- gethit:
	- use binary search to find the first hit in our window
	- then num hits is just length of rest of array



## #queue

- strat
	- store each hit separately
	- delete

- calls are being made to the system in chronological order (i.e., `timestamp` is monotonically increasing)
	- so we only ever need to store hits from past 300 seconds
	- remove earlier hits outside the time window
		- keep popping until time stamp is within 5 min

- gethits is just length of the queue

- cons
	- inefficient space: if lots of hits within the time window, then queue becomes large


## #ad_hoc 
- see the optimal/array solution below
- i don't think this is adhoc
	- i suspect there is something here about, certain types of questions, can cleverly use a fixed array + operations (such as mod) instead of a hashmap + 'inefficient' routines


# Strategies


## array + bsearch: key hits, never delete

- note that this has optimal time until `log n > 300`
	- aka `n > 2^300`
	- so yes, this really is optimal time for any reasonable application
	- !! wait...the queue approach below should have comparable time?
		- bc gethit is O(1) once u pop everything out of the window
		- and popping out of the window..can probly do some amortized analysis to explain why this is efficient as well?


- complexity
	- n = num hits
	- time
		- O(1) for hits
		- O(log n) for gethits
	- space
		- O(n)

```python
class HitCounter:
    def __init__(self):
        self.ts = []

    def hit(self, timestamp: int) -> None:
        self.ts.append(timestamp)

    def getHits(self, timestamp: int) -> int:
        return len(self.ts) - bisect_left(self.ts, timestamp - 300 + 1)
```





## queue: key hits, manually delete

- alternatively/also
	- why not also delete stuff on insert?
		- ie also run the 'pop from queue if outside window' routine
			- bc this in theory would be more space efficient
		- tradeoff is worse time...i suspect, even with amortized complexity analysis, then insert still wouldnt be O(1) anymore


```python
class HitCounter: # queue
    def __init__(self):
        self.hits = deque()

    def hit(self, timestamp: int) -> None:
        self.hits.append(timestamp)

    def getHits(self, timestamp: int) -> int:
        while self.hits and self.hits[0] <= timestamp - 300:
            self.hits.popleft()
        return len(self.hits)
```


## dict: key times, never delete

- don't store each hit individually
	- instead, use a dict to store (timestamp, numHits at that time)

- gethits
	- sum hits corresp times that occured in window
	- 2 ways to gather the correct times:
		- iterate over all keys, and ignore if outside window
		- iterate over all times in window and check keys


- complexity
	- let m = num unique timestamps
	- time
		- O(1) to record hit
		- gethits: depends: O(m) or O(300)
	- space
		- O(m)
			- bc we store every timestamp's hit count

- optimization: delete keyvals outside window?
	- (imo this strat should be 'never delete', bc adding a 'delete routine' here isn't really an overall optimization...there's likely serious time costs)
	- (note, this could also be applied to the array solution above)
		- well actually maybe not? iirc deleting from front of the array...probly not efficient?
	- for space efficiency at the cost of time
	- imo you could kinda do this efficiently:
		- var to tell you up to what point you've deleted
			- aka min key in counter
			- so you only have to start deleting from this point
		- when u want to delete: iterate through times starting from 'curr min key' to 'curr window start', and delete keyvals that exist
	- design decision: when would you run the 'cleaning/delete' operation?
	- (don't think about this too hard tho...move onto the 'buckets' solution below, just memorize that it's optimal)



```python
class HitCounter:
4    def __init__(self):
8        self.hits = Counter()
9
10    def hit(self, timestamp: int) -> None:
17        self.hits[timestamp] += 1
18
19    def get_hits(self, timestamp: int) -> int:
27        return sum(count for time, count in self.hits.items() if time > timestamp - 300)
```





## queue: key times, manually delete
- queue of lists
	- (timestamp, hits)
- bc problem says monotone timestamps
	- so incoming hit:
		- either modify node at end of queue
			- if same timestamp
		- or add a new node
			- if later timestamp
- manually pop from front of queue as needed
- this has overall worse time compared to the other queue approach?
	- popping is quicker bc one timestamp can pop multiple hits
	- but gethits is slower bc sum over all timestamps, instead of just length of queue



## array: key times, 'auto' delete



- 2 arrays of size 300
- getHits
	- iterate through the times array to find all the positions within 5 min
	- sum vals


- (optimal space strat)
	- out of strats with optimal space: this has best time
- complexity
	- time
		- O(1) for hit
		- O(300) for gethits
	- space
		- O(300)


- #/jargon ?
	- 'buckets'
- 2 clever things going on:
	- 'times' array lets u know when to reset the hit counter in the hits array
		- ie tells us the timestamp associate with the same index in hits array
			- thus, we know whether to increment or reset to 1
		- timestamp[i] stores the timestamp of the last counted hit
			- if timestamp[i] == timestamp-300 , this count should be reset
				- or more simply: timestamp[i] > timestamp
			- if timestamp[i] == timestamp, then increment hits[i]
	- mod by 300
		- to find where to increment the hit
		- automatically deal with 'window is at most size 300'
			- versus hashmap cannot do this
			- imo there's an idea here where like...if ur keys are just straight ints, then see if there's a clever way to use an array instead..ie buckets
			- and this use of mod operation might be another very standard trick






- is there a way to 'parallelize' the gethits function?
	- apparently maybe 'map reduce' can sometimes be used in a similar way? (but probably not)
	- probly will need a 'lock'
		- i vaguely remember this jargon for when i was looking into 'parallel'/numba




```python
class HitCounter:
    def __init__(self):
        self.times = [0] * 300
        self.hits = [0] * 300

    def hit(self, timestamp: int) -> None:
        idx = timestamp % 300
        if self.times[idx] != timestamp:
            self.times[idx] = timestamp
            self.hits[idx] = 1
        else:
            self.hits[idx] += 1

    def getHits(self, timestamp: int) -> int:
        res = 0
        for i in range(300):
            if timestamp - self.times[i] < 300:
                res += self.hits[i]
        return res
```


[632. Smallest Range Covering Elements from K Lists](https://leetcode.com/problems/smallest-range-covering-elements-from-k-lists/)

```python
class Solution:
    def smallestRange(self, nums: List[List[int]]) -> List[int]:
        
```

# Description

You have `k` lists of sorted integers in **non-decreasing order**. Find the **smallest** range that includes at least one number from each of the `k` lists.

We define the range `[a, b]` is smaller than range `[c, d]` if `b - a < d - c` **or** `a < c` if `b - a == d - c`.

**Example 1:**  
**Input:** `nums = [[4,10,15,24,26],[0,9,12,20],[5,18,22,30]]`  
**Output:** `[20,24]`  
**Explanation:**
```
List 1: [4, 10, 15, 24,26], 24 is in range [20,24].
List 2: [0, 9, 12, 20], 20 is in range [20,24].
List 3: [5, 18, 22, 30], 22 is in range [20,24].
```

**Example 2:**  
**Input:** `nums = [[1,2,3],[1,2,3],[1,2,3]]`  
**Output:** `[1,1]`  

**Constraints:**
- `nums.length == k`
- `1 <= k <= 3500`
- `1 <= nums[i].length <= 50`
- `-10^5 <= nums[i][j] <= 10^5`
- `nums[i]` is sorted in **non-decreasing** order.

---

# todo

#/meta 
- chatgpt explanations are not reliable
	- on some algomonster explanations, when there are multiple popular strats to solve the problem, the chatgpt will sometimes mention all of them (or the wrong one), even when the actual code uses just one/different approach
	- example:
		- https://algo.monster/liteproblems/632
		- mentions heap/pq, which is a popular strat for this problem, but is not actually used in the code

- https://www.youtube.com/@happycoding1093
	- seems reliable enough to get the big idea across
	- i'm p sure he's korean
	- fairly concise
	- raises interesting questions along the way, ie 'why didnt we use a different structure'


- https://www.youtube.com/@IDeserve/videos
	- seems to have nice visuals

- #/fundy 
	- understand differences/tradeoffs/definitions btwn:
		- pq
		- monotone queue
			- monotone stack
		- https://en.wikipedia.org/wiki/Monotone_priority_queue
			- ?? when is this even relevant
	- i suspect:
		- for pq vs mq, it just depends on whether you expect your insertions to be arbitrary, or always a 'new max'
			- if new max, then probly just use mq cuz u know it just goes to the end
			- but if it's arbitrary: then mq seems very inefficient, bc worst case u need to binary search the array to see where it goes...and then put it in and then shift all the array entries...or i guess you could use link list
			- wait.. this seems like same complexity as the heap insertion operation?? wtfam i getting wrong


#binary_search 
- i suspect:
	- if given a single sorted array, for certain types of problems:
		- (figure out the common types)
		- ie one would be like: 'find a specific element that meets this condition'
	- then you should immediately try to think if a binary search technique works
		- or if unsorted, and no obvious 'greedy' algo: think if you can sort, then do binary search


#/jargon 
figure out, when exactly should i use pq tag vs heap tag



# References

# #sliding_window 




## #greedy/priority_queue  

### identifying patterns
- as usual, we have:
	- a collection of sorted containers
- and so i suspect: should always be suspicious of a greedy+pq approach will work
- imo a distinction has to be made here:
	- here the input is technically a 2d array, but it is not:
		- (very strong structure) sorted if flattened
		- (fairly strong structure) sorted along each dimension
			- imo less strong than 'flattened', bc 'flattened' is a subset of this
	- so we cannot just do some kind of 'binary search' technique along the whole array
		- also doesn't even make sense tbh, bc the problem wants to return something that requires considering every container simultaeneously
	- here, the input is sorted only along 1 dimension
- the above discussion makes me think...i can very precise yet abstract about understanding when i should consider a greedy + pq approach
	- and also, if given unsorted '2d' input:
		- see if sorting along 1 dimension (ie sorting within each 'container') will allow for a greedy + pq approach
	- like #binary_search ....i think being precise/abstract here will also require understanding what 'kind' of problems are amenable to greedy+pq...ie the nature of the solution they are asking (aka go figure out proper #/jargon..ie optimization, 'selection', etc)
- also imo the above discussion is worth it imo...bc i think it will be good to understand why i should immediately use a 'brute force + binary search' approach as a last resort:
	- ie: it's true that each container is sorted...so technically, for a given range, you can pretty efficiently check whether it contains an element of each list by binary search on each list
		- see approach 2 https://leetcode.com/problems/smallest-range-covering-elements-from-k-lists/editorial/
		- 

- also remember:
	- each container sorted + need to involve every container:
		- do not default to flattening + sorting!!!
			- bc you waste time sorting
			- waste space by building a flattened container
			- if you still need to keep track of the associated og container, you waste even more space bc each 'element' in flattened list now needs to be converted to a 'node' of the form (element, id of og list)
			- (though apparently this can work..uses a modification to solution of:)
				- #/fundy https://leetcode.com/problems/subarrays-with-k-different-integers/description/
		- aka pq is just very natural to use here

### strat

- here:
	- pq will contain an element from each list
		- so initially: contains the front of each list
		- highest priority is the smallest element
	- so: when popping from pq, we need to know which list it came from
		- so that we can insert the next element from the same list
		- aka: pq nodes are at least: (elt value, index of the element's list)
			- for efficiency: i bet you should also store the "index of the element in it's list"
				- so we don't have to run a subroutine to figure out what the next element to push
				- also makes it easy to handle when we can't push; see below
	- at each step we:
		- determine smallest range that covers the pq
			- to make this efficient:
				- have a global var that stores currMax of pq
					- clearly only needs to be updated when we push into the pq
		- if needed, update 'curr smallest range so far'
		- pop
		- push if possible
			- if we popped an element that was the end of it's list; then that means we cannot push anything
				- so the routine ends


### classic?
- is this a variant of a classic problem?
	- doesnt really look like same 'subpattern' as halim1's 'greedy with pq'
	- !!! see discussion below, this actually might be a #greedy/sweep_line problem
	- 


### (pruning) why update min ; aka correctness
- why update min:
	- the idea is: we would like to check 'the next candidate' that 'could minimize the range'
	- 2 ways this could happen:
		- replace max elt with something smaller in its list
		- replace min elt with something larger in its list
	- however the point is:
		- if we started with all min elts:
			- then updating using first rule doesn't make sense because...we've already pruned this candidate somehow?
				- !!! i suspect 'pruning candidates' is the correct jargon/idea here
			- or actually maybe to realize the correctness:
				- after initialization, it's obvious we can't pick the first rule 
					- the maximum value can't be reduced any further, since it already corresponds to the minimum value in one of the lists. Reducing it any further will lead to the exclusion of all the elements of this list(containing the last maximum value) from the new range.
				- and now, slightly modify the above reasoning for further iterations:
					- ??
			- i think s what this guy is saying, but not well:
				- Since all lists are sorted, suppose an answer that can be got by moving back max pointer, we would have already taken that into account. The fact that max pointer got to where it was is cuz it was at the minimum some point in time


- !!!!! why we update min
	- yes, i need to think of this like pruning:
		- there's no point looking at any other 'combos' that involve the min
		- because vaguely: no other 'combo' involving this min, can generate a smaller range
			- something about the other elements being 'curr mins' of their respective lists
			- this reasoning is obvious/correct for the base/initial case
				- so what about 'during rest of iterations'? 
	- btw ok, i tihnk other explanations are not the right inspo
		- the intuition is correct, in that we should look at options that have potential to reduce the range
		- but it's poor/incomplete inspo bc they don't explain why we can 'ignore' states/'combos'
	- anyways, back to the train of thought:
		- correctness for non-initial case:
		- !! yes, this is the min range involving this min element:
			- it's bc all other elts are:
				- smallest elts in their list, that are geq the min elt
					- 'geq the min elt' is obvious...bc thats why this is 'the min elt'
					- and there isn't smaller elt in their list that is also geq the min elt:
						- bc means, at some point we moved from smaller to the curr elt
						- but this is impossible if smaller elt is geq than min elt
						- bc only time we move something, is if it is the min elt
						- wait..what if smaller elt is equal to min elt?
							- !!!!
							- i think i'm close to explaining this..there's like a cleaner way though, so that this part isn't hard to explain
							- oh actually, this is kinda clean:
								- basically can just ignore this current iteration
								- when that other smaller elt was popped: its min range contains the min range of the curr min elt we're popping
							- !!!! so actually imo: might be better to:
								- say that the strat is really: at each iteration, pop all the elts equal to min value (there might be multiple), and update with next elt in list
									- bc this aligns more with the greedy intuition
									- we've alrdy found min ranges for these elts
								- but then say: for simplicity of implementation: could just pop and analyze one step at a time
								- wait..this feels like #greedy/sweep_line 
									- the line is like sweeping across all the lists
										- ie if each list is horizontal, from least to greatest: then imagine a vertical line sweeping from left to right
										- and the list elements are also spaced so that they're in 'correct order' wrt to elts of other lists
									- after each 'computation', we move line to the next 'min value' it sees. then to compute min range: its just the 'max' over the 'closest things to the right' of the line. both of processes are efficiently done via the pq structure (to figure out where to move hte line next) and a global var holding max in the pq.
			- obvi, bc these other elts satisfy this property:
				- then this is the min range involving this element
				- so no point looking at other combos involving this elt
				- so we update this element to its next in the list
					- crucially: this preserves the 'invariant' for the next min elt

### starting from 'sweep line' approach
- we need some structures that:
	- holds exactly 1 element from each list
		- imo this is the key property that makes you use heap, and not some other structure that allows for 'efficient sorting'
		- basically, we dont care about other elements in each list; we only care about 1
	- helps us quickly update and compute the max and min
	- in particular, we will 'delete' the min at every iteration
		- ie pop
		- ie use min heap
	- only 1 new elt is added every time
		- so just use global var for the max




### btw, symmetric approach exists
- the right bound can also be value in array
- so you could basically do same strat in reverse
	- init the heap with last entry of each array, and then sweep left


### sweep line and pq+greedy on k containers
- i suspect that each is an 'important example' of the other
- is one a special case of the other?
	- maybe sweep line might be the more general idea!
	- iirc: the halim1 'pq+greedy' is NOT on k containers
		- rather its on like k 'bins'
		- wait ok, then 'containers' is definitely not the right thing to say here
		- i was trying to be 'formal' bc 'containers' is the jargon that python uses to describe lists/sets/etc
			- but tbh, container jargon makes no sense...it'll always be lists i bet


### !!!why we look at 'min range of min elt'
- this actually makes complete sense
	- oh actually, is also important for correctness as well
- the smallest range:
	- the lower bound should obvi be equal to an elt in one of the lists
- so it suffices to consider:
	- minimize over: for each elt:
		- 'smallest range where it is the min elt'
- and also this makes 'exhaustion' corectness obvious:
	- if an elt is larger than largest elt of one of the lists:
		- then there's no point including this elt in the above search


### (meta) do not try to find 'inspo' for greedy anymore

#/meta 
- moving forward:
	- imo do not try to explain/prove correctness of greedy
	- just understand the structures used + what is the 'greedy decision' we make
	- for understanding 'correctness' or how to 'realize it':
		- imo it's efficient to gamble on inductive brain magic
	- the hardest time to explain something is now, bc i'll be better in the future
		- for most strats: they're still easy enough to explain
		- greedy is likely the only exception, bc its so 'adhoc'
			- aka it's just very inefficient to do this rn

btw #/move, this should go into a 'greedy' category imo...not heap
- esp bc iirc #sliding_window can be considered 'greedy' as well

#/meta : when #move problems:
- maybe i shoulod think about, what about the optimal strat do i learn the 'most' about
	- probly, the most 'difficult' part to understand
	- or like what aspect is it 'showcasing' the most
	- sometimes it will be a certain structure; other time it will be a paradigm; etc




### pruning: exhaustion

- another big point here is:
	- the only elts we never consider the 'min range where it is the min elt' are elts that never get popped from the pq
	- aka i need to understand why we can ignore all these
	- vaguely:
		- bc all of those 'skipped elts': we know the min range containing that elt(not even the min range where it is the min elt) will be larger than the range found during the last iteration
			- bc those elts are larger than the last elt of a list:
				- we know that the range for that elt: it needs to overlap every list, and so optimal choice is to include the last elt of that list
				- ie the min of that range has to at least be lower than that last elt


### exhaustion correctness
- why we end at exhaustion:
	- if any of the lists gets completely exhausted, it means that the minimum value being increased for minimizing the range being considered can't be increased any further, without causing the exclusion of all the elements in atleast one of the lists. Thus, we can stop the search process whenever any list gets completely exhausted.



### complexity
- complexity
	- time
		- O(n log m)
		- Heapification of m elements requires O(log(m)) time. This step could be done for all the elements of the given lists in the worst case. Here, n refers to the total number of elements in all the lists. m refers to the total number of lists.
	- space
		- O(m) for pq



# Strategies

## brute force + binary search
- approach 2
	- https://leetcode.com/problems/smallest-range-covering-elements-from-k-lists/editorial/

## flatten + sort + sliding window

#/cleanup move these 3headings to refs or tag index

### relationship with pq
- imo important thing to highlight here is why you shouldn't use this approach
	- (assuming pq is better..ie better constant or more 'natural idea')
		- i suspect it's same time complexity
	- definitely worse space complexity, so this already is a good reason
	- ie why pq is more natural?
		- bc imo, this is an important point as well..ie this just feels like it fits in the standard pattern of 'use pq'
		- well imo the issue is..how do you even come up the 'greedy' idea? nobody seems to justify why we can be 'greedy' when explaining the pq solution

- also keep in mind: the optimal way you flatten and sort is with pq anyways
	- [[23. Merge k Sorted Lists]]
	- i suspect:
		- if your algo is just:
			- merge+sort
			- then go through flattened sorted in 1 pass, to compute answer
		- then it really seems, there's likely no need to actually create the flattened sorted array
		- bc the pq essentially provides those elements in order
		- almost like the pq is the 'iterable' version of the flattened+sorted structure
	- and imo i feel like: '1 pass without storing much' often signifies some kind of greedy property
		- so think if you can do pq + greedy

### code examples (and why/how flatten+sliding window)

- https://leetcode.com/problems/smallest-range-covering-elements-from-k-lists/solutions/643888/python-two-pointers-hash-table-solutions-with-explanations/
	- ok, now i see how flattening + sliding window feels natural
	- i do agree, this really can be interpreted as [[76. Minimum Window Substring]]
		- 'use all lists' in this problem, is essentially the 'target substring chars' in 76
	- btw: `seen` is only so that `v` contains unique elements
		- not actually necessary, but makes the sorting step more efficient
			- at the cost of...flattening is more expensive
		- (no point to include duplicates when we flatten)
		- (and btw, constraints allow for duplicate entries in the input)
		- but you really don't need a separate data structure for this...can just add a condition to skip elt if we just saw it in the array
- https://algo.monster/liteproblems/632
	- same thing
- https://leetcode.com/problems/smallest-range-covering-elements-from-k-lists/solutions/4312120/c-python-sorting-sliding-window-solution-with-explanation/
	- can use an array instead, bc you're storing (index:count)
		- #/jargon btw what is this called again? like a 'hashmap' that really is just an array bc your keys can easily just be assigned to array indices?


### 2p vs SW; 2p vs 'iteration'
- btw #/jargon #iteration/two-pointer #sliding_window 
	- imo this is 2 pointer?
		- move left pointer if every list index is present
		- move right pointer if not every index is present
	- ehh maybe not...tbh, this seems like such an important variant of 2pointer, that i should call it sliding window..especially bc that's what everyone else calls it
		- aka why make up my own jargon 
	- ok so maybe, all these 'iteration' subtags should be renamed to subtags of two pointer, and the subtag name itself should be what it's commonly referred to?
		- and in the discussion, i can just explain that the 'general concept' is 'iteration for X reason'
		- otoh..i'm not a fan of 2pointer bc it's specifically on 1d arrays
		- and i feel like this concept can definitely be applied more generally
	- imo do both
		- the 'iteration' optimizations: they're so often applied to 1d array problems, that it's just more convenient/established to use the 2 pointer jargon
		- if/when i find problems that use same 'idea' on more general input, then i put the 'iteration' tag
		- and then in the discussion of '2pointer' (subtags), i can mention that it's a specific case of the 'iteration' (subtag)
			- and vice versa: metnion in the 'iteration' subtags that THE important/common/standard example is the corresp '2pointer' subtag


### 1 pass over flatten sort -> pq + greedy
- !!!!
	- understanding the flatten+sort+'1 pass to compute' approach probably has serious value:
		- will help make it really clear why the pq +greedy algo is correct
			- or even better, coming up with a pq+greedy algo myself
	- very straightforward to see why the 'sliding window' approach is correct,
		- bc you're just pruning windows, see other similar problems
		- and point is: the pq+greedy approach is implicitly doing this
			- it's just far more complicated to see, bc we're not working with a 1d array

	- actually, i dont think the 'sliding window' info itself helps you 'figure out how to prove greedy pq'
		- i think its more: 'sliding window is correct bc pruning' probly helps you realize that 'prove greedy pq via some kind of pruning argument'


- figuring out + proving correctness of a '1 pass over flatten sort' algo is likely easier than pq + greedy
	- particuarly the 'proving correctness' part
	- btw when you 'flatten sort':
		- default to making sure the flattened array:
			- each elt also knows which og array it came from
		- and point is: when you look at ur algo, and it seems that the 'og array info' is actually crucial
			- this should be another big hint to look into a pure pq+greedy approach
			- bc pq maintains 'og array info' as well
		- if you don't actually need 'og array info'...then it's possible you could probly come up with an even simpler algo?
			- ie binary search
		- oh actually, i think this observation here should be applied in reverse:
			- if og array info is crucial, then you probly dont want to (sort +) binary search

- #/meta i think i'm dumping too many thoughts here...
	- imo less typing, more gambling on 'inductive magic of the brain'


## #priority_queue 


```python
def smallestRange(self, A):
    pq = [(row[0], i, 0) for i, row in enumerate(A)]
    heapq.heapify(pq)
    
    ans = -1e9, 1e9
    right = max(row[0] for row in A)
    while pq:
        left, i, j = heapq.heappop(pq)
        if right - left < ans[1] - ans[0]:
            ans = left, right
        if j + 1 == len(A[i]):
            return ans
        v = A[i][j+1]
        right = max(right, v)
        heapq.heappush(pq, (v, i, j+1))
```


```python
import heapq
class Solution:
    def smallestRange(self, nums: List[List[int]]) -> List[int]:
        result = []
        hq = []
        high = -sys.maxsize
        for i in range(len(nums)):
            heapq.heappush(hq, (nums[i][0], i, 0))
            high = max(high, nums[i][0])
        result = [hq[0][0], high]
        while True:
            num , listIndex, numIndex = heapq.heappop(hq)
            if numIndex == len(nums[listIndex]) - 1:
                break;
            nextNum = nums[listIndex][numIndex + 1]
            heapq.heappush(hq, (nextNum, listIndex, numIndex + 1))
            high = max(high, nextNum)
            if high - hq[0][0] < result[1] - result[0]:
                result = [hq[0][0], high]
        return result
```



```python
import heapq

class Solution:
    def smallestRange(self, nums):
        """
        :type nums: List[List[int]]
        :rtype: List[int]
        """ 
        heap = [(List[0], index, 0) for index, List in enumerate(nums)]  
        # First element of each list with list number and index of the element in each list
        heapq.heapify(heap)                                              
        # to get the min value from all the elements of each list
        
        max_val = max([List[0] for List in nums])                        
        # To get the max value for the range
        smallest_range = -1e9, 1e9                                       
        # max and min for the range
        
        while heap:
            min_val, list_index, num_index = heapq.heappop(heap)         
            # get the min value, the list its from and the index it is at in the particular list
            
            if max_val - min_val < smallest_range[1] - smallest_range[0]:  
            # To find the smallest difference which will be the range
                smallest_range = min_val, max_val
                
            if num_index + 1 == len(nums[list_index]):                   
            # once any one of the list is exhausted, return the range
                return smallest_range
            
            next_num = nums[list_index][num_index+1]                     
            # To get the next element from the list that has the min value
            
            max_val = max(max_val, next_num)
            heapq.heappush(heap, (next_num, list_index, num_index+1))
```
